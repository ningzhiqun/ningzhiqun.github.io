{"./":{"url":"./","title":"我的","keywords":"","body":"来自杭州的一位程序员 "},"doc/practice/":{"url":"doc/practice/","title":"系统设计思考","keywords":"","body":"记录平时敲代码时的一些思考 "},"doc/business/longtoshort.html":{"url":"doc/business/longtoshort.html","title":"长连接转短链接","keywords":"","body":"前言 最近系统上线了转介绍功能，为了隐藏一些敏感信息，且短信基于字符收费。于是花了点时间做了一个短链服务，实现思路其实很简单，这里简单介绍下实现细节，以及一些优化过程。 功能简单描述 功能很简单，实现将长网址缩短的功能，如： 为什么要转短链？因为要控制每条短信的字数，对于公司来说，短信里面的字可都是钱呀。 为什么不用 t.cn，url.cn 等短链服务呢，它们生成的链接不是更短吗？是的，它们确实能实现更短的链接，可是要收钱的，而且这里面充满了商业数据呀。 短链服务总的来说，就做两件事： 将长链接变为短链接，当然是越短越好 用户点击短链接的时候，实现自动跳转到原来的长链接 长链转短链 在转短链的时候，我们其实就是要将一个长长的链接映射为只有 4 到 7 个字母的字符串。这里我用了 MySQL 来存储，存放 short_key 和 original_url 的记录。 数据表很简单，最主要的列有以下几个： id: 逻辑主键，BIGINT short_key: 短链中的字符串，域名部分一般不需要加进去，加入唯一索引 unique original_url: 原长网址，限 256 字符 另外，基于业务需要，可以加入业务标识 biz、过期时间 expire_time 等。 在生成 key 的时候，一种最简单的实现方式是使用随机字符串，因为是随机码，所以可能会出现失败，通常就需要重试。随着记录越来越多，就越容易发生 key 重复的情况，这种方案显然不适合数据量大的场景。 我们不容易保证我们随机生成的 key 不重复，但是我们容易实现的就是 id 不重复，我们只要想个办法把 id 和 key 一一对应起来就可以了。 单表场景，直接使用数据库自增 id 就能实现 id 唯一。多库多表，大家肯定都有一个全局发号器来生成唯一 id。 直接将 id 放在短链上可以吗？这样就不需要使用 key 了。功能上是没有问题的，不过问题就是还是会太长，然后由于 id 通常都是基本自增的，会引发很多问题，如被别人用一个简单的脚本给遍历出来。 接下来，我们讨论怎么将 id 变为 key。 在短链中，我们通常可以使用的字符有 a-z、A-Z 和 0-9 共 62 个字符，所以，接下来，我们其实就是要将 10 进制的 id 转换为 62 进制的字符串。 转换方法很简单，大家都学过二进制和十进制之间的转换，这里贴下简单的实现： private static final String BASE = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"; public static String toBase62(long num) { StringBuilder sb = new StringBuilder(); int targetBase = BASE.length(); do { int i = (int) (num % targetBase); sb.append(BASE.charAt(i)); num /= targetBase; } while (num > 0); return sb.reverse().toString(); } 这样，十进制的 id 总是能生成一个唯一的 key，同样地，我们也可以通过 key 还原出 id。 在分库分表的时候，我们可以选择使用 id 来做分表键，也可以使用 key 来做分表键。如果是使用 id 的话，因为前端过来都是 key，所以需要先将 key 转换为 id。这里我们将使用 key 做分表键。 本文不会用到 62 进制转 10 进制，不过也贴出来让大家参考下吧： public static long toBase10(String input) { int srcBase = BASE.length(); long id = 0; String r = new StringBuilder(input).reverse().toString(); for (int i = 0; i 短链转长链 这一步非常简单，用户点击我们发给他们的短信中的短链，请求发送到我们的解析系统中，我们根据 key 到数据库中找原来的长链接，然后做个 302 跳转就可以了。 这里贴下 Spring MVC 的代码： @GetMapping(\"/{key}\") public String lookup(@PathVariable String key) { String originalUrl = shortenerService.lookup(key); if (StringUtils.isBlank(originalUrl)) { // 如果没有找到长链接，跳转到我们的 m 站，这里其实定制一个 404 页面比较好 return \"redirect:https://m.zhongan.com\"; } return \"redirect:\" + originalUrl; } 细节优化 1、加入随机码 62 进制用更短的字符串能表示更大的数，使得我们可以使用更少的字符，同时不会让用户直接知道我们的 id 大小，但是稍微懂一点技术的，很容易就能将 62 进制转换为 10 进制，在行家眼里，和直接使用 id 没什么区别。 下面，我们就来优化这部分。 首先，上面的代码中，我们可以打乱这个 BASE 字符串，因为如果不打乱的话，那么 62 进制中就会有 XXb = XXa + 1，如 10 进制的 999998 和 999999 转换为 62进制以后，分别为 4C90 和 4C91，大家是不是发现有点不妥。 接下来，我们可以考虑加随机字符串，如固定在开头或结尾加 2 位随机字符串，不过这样的话，就会使得我们的短链活生生地加了 2 位。 这里简单介绍下我的做法，使得生成的 key 不那么有规律，不那么容易被遍历出来。 我们得到 id 以后，先在其二进制表示的固定位置插入随机位。如上图所示，从低位开始，每 5 位后面插入一个随机位，直到高位都是 0 就不再插入。 一定要对每个 id 进行一样的处理，一开始就确定下来固定的位置，如可以每 4 位插一个随机位，也可以在固定第 10 位、第 17 位、第 xx 位等，这样才能保证算法的安全性：两个不一样的数，在固定位置都插入随机位，结果一定不一样。 由于我们会浪费掉一些位，所以最大可以表示的数会受影响，不过 64 位的 long 值是一个很大的数，是允许我们奢侈浪费一些的。 还有，前面提到高位为 0 就不再插入，那是为了不至于一开始就往高位插入了 1 导致我们刚开始的值就特别大，转换出来需要更长的字符串。 这里我贴下我的插入随机位实现： private static long insertRandomBitPer5Bits(long val) { long result = val; long high = val; for (int i = 0; i > pos; result = ((high >> (64 - pos))); } return result; } 这样，我们 10 进制的 999998 和 999999 就可能被转换为 16U06 和 XpJX。因为有随机位的存在，所以会有好几种可能。到这里，是不是觉得生成出来的字符串就好多了，相邻的两个数出来的两个字符串没什么规律了。 另外，建议 id 从一个中等模式的大小开始，如 100w，而不是从 1 开始，这个应该很好理解。 2、加入缓存 为了提高效率，我们应该使用适当的缓存，在系统中，我分别使用了一个读缓存和一个写缓存。 通常，我们使用读缓存 (key => originalUrl) 可以获得很多好处，大家想想，如果我们往一批用户的手机发送同一个短链，可能大家都是在收到短信的几分钟内打开链接的，这个时候读缓存就能大大提高读性能。 至于写请求，接口来了一个 originalUrl，我们不能去数据库中查询是否已经有这条记录，所以两条一模一样的链接我们会生成两个不一样的短链接，当然，通常我们也是允许这种情况的。 这里我指的是在分库分表的场景中，我们只能使用 key 来查找，已经不支持使用 original_url 进行数据库查找了。 由于存在短时间内使用两条一模一样的长链接拿过来转短链的情况，所以我们可以维护一个写缓存 (originalUrl => key)，这里使用 originalUrl 做键，如设置最大允许缓存最近 10000 条，过期时间 1 小时，根据自己实际情况来设置即可。这里写缓存能不能提高效率，取决于我们的业务。 由于生成短链的接口一般是提供给其他各个业务系统使用的，所以其实可以由调用方来决定是否要使用写缓存，这样能得到最好的效果。如果调用方知道自己接下来需要批量转换的长链是不会重复的，那么调用方可以设置不使用缓存，而对于一般性的场景，默认开启写缓存。 3、数据库大小写 这里再提最后一点，也是我自己踩的坑，有点低级失误了。一定要检查下自己的数据表是不是大小写敏感的。 在大小写不敏感的情况下，3rtX 和 3Rtx 被认为是相同的。 解决办法如下，设置列为 utf8_bin： ALTER TABLE `xxx` MODIFY `short_key` CHAR(10) CHARACTER SET utf8 COLLATE utf8_bin; 性能分析 这个系统非常简单，性能瓶颈其实都集中在数据库中，前面我们也说了可以通过缓存来适当提高性能。 这里，我们不考虑缓存，来看下应该怎么设计数据库和表。 首先，我们应该预估一个适当的量，如按照自己的业务规模，预估接下来 2 年或更长时间，大概会增长到什么量级的数据。 如预估未来可能需要存放 50-100 亿条记录，然后我们大概按照单表 1000w 数据来设计，那么就需要 500-1000 张表，那么我们可以定 512 张表，512 张表我们可以考虑放 2 个或 4 个库。 我们使用 key 来做分表键，同时在 key 上加唯一索引，对于单表 1000w 这种级别，查询性能一般都差不了。 我没有在生产环境做过压测，测试环境中使用单库 2 张表，在不使用缓存的情况下，写操作可以比较轻松地达到 3000 TPS，基本上也就满足我们的需求了。本来测试环境各种硬件资源就和生产环境没法比，更何况我们生产环境会设置多库多表来分散压力。 （全文完） "},"doc/design/":{"url":"doc/design/","title":"设计模式","keywords":"","body":"设计模式 设计模式（design patterns），是指在软件设计中，被开发者反复使用的一种代码设计经验。 使用设计模式的目的：1、提高代码的可复用性；2、提高代码的可扩展性；3、代码易阅读。 为什么要使用设计模式？ 根本原因还是在软件开发中要实现可维护、可扩展，就必须尽量抽象出可复用性的代码，降低代码质检的耦合度（松偶合）。 设计模式是基于OOP编程思想提炼出来的，它主要有以下几个原则： 1、单一职责原则：单一职责原则表示一个模块的组成元素之间的功能相关性。从软件变化的角度来看，就一个类而言，应该仅有一个让它变化的原因；通俗地说，即一个类只负责一项职责。 2、开放-关闭原则：对现有功能扩展开放，对现有功能修改关闭。 3、依赖倒转原则：高层模块不应该依赖低层模块，二者都应该于抽象。进一步说，抽象不应该依赖于细节，细节应该依赖于抽象。 4、里氏替换原则：在软件开发中，新增功能时不要影响既有功能，也不要覆盖既有方法，尽量兼容。 设计模式按按功能分类可分为三种： 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 行为型模式 结构型模式 创建型模式 策略 代理 单例 命令 适配器 原型 职责链 桥接 工厂 状态 装饰 建造 观察者 外观 中介者 享元 迭代器 组合 访问者 备忘录 GoF的23种设计模式的功能： 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 "},"doc/design/design.html":{"url":"doc/design/design.html","title":"24 种设计模式","keywords":"","body":"记得刚开始工作的时候，看到网上说设计模式，每次看的都是云里雾里的样子。现在工作3年多了，现在回过头来看设计模式，突然有种顿悟的感觉。（ps：作为一个过来人的经验告诉大家，如果你刚工作不久，现在看了设计模式，可能不是很理解，等你积累了一定的工作经验的时候，回过头来看，会觉得 so easy ！！！）。好了，废话不多说，接下来直奔主题。 设计模式本质上是前人工作中，对自己的代码进行的抽象总结，其中被大家广泛认同的是 Gang of Four（GOF）的分类了。他们将设计模式分为23中经典的模式。根据用途我们有可以将其分为三大类：创建型模式、结构式模式和行为模式。（大家要相信，即使你不看，如果一直从事程序猿工作，会在日常工作中用到某些模式） 创建者模式比较容易理解，如果你是一位从事 Java web 开发，spring core 就建立在这上面。 创建型模式 创建型模式关注的核心目标就是如何创建一个对象，即关注的核心是类的创建过程。我们在日常开发中会经常面对大量对象的创建或者复杂对象的创建，而创建型模式就是帮助我们解决这种问题的。 创建型模式分为：1、工厂方法；2、抽象工程；3、建造者；4、原型模式；5、单例模式。 简单工厂模式 这个很简单，简单都不知道怎么描述，先上代码： public class FruitsFactory { public static Fruit getFruit(String name) { // Apple 和 Banana 都继承了 Fruit 接口 switch (name){ case \"apple\": retrun new Apple(); case \"banana\"： return new Banana(); default: return null; } } } 这样就能清楚的看到，我们只需要定义一个xxxFactory 工厂类，里面有一个方法，根据我们的需要去获取不同实现类。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工程方法模式 简单工厂模式是简单，但是很多情况下它都有着它的局限性，比如常常我们需要两个或者两个以上的工厂。这个时候也就引申出我们的——工程方法模式。 public interface FruitsFactory { Fruit getFruit(String name); } public class ChineseFruitFactory implements FruitsFactory { @Override public Fruit getFruit(String name) { // ChineseApple 和 ChineseBanana 都继承了 Fruit 接口 switch (name){ case \"apple\": retrun new ChineseApple(); case \"banana\"： return new ChineseBanana(); default: return null; } } } public class AmericanFruitFactory implements FruitsFactory { @Override public Fruit getFruit(String name) { // AmericanApple 和 AmericanBanana 都继承了 Fruit 接口 switch (name){ case \"apple\": retrun new AmericanApple(); case \"banana\"： return new AmericanBanana(); default: return null; } } } 虽然看起来两个工厂很像，但是，不同的工厂生产的东西是不一样的。相较于简单工厂模式，工厂方法模式多了一步选择合适工厂的步骤。 工厂方法模式的核心在于选择合适的工厂。比如，Java 核心库提供了标准 JDBC 的接口，但是它的实现有 MySQL 、SQL server 等实现，分别将数据写入到不同的数据库中。因此在我们日常的开发 springboot 应用中，第一步就是选择合适的数据库连接。 抽象工厂模型 大家都知道，一台可供程序猿勉强使用的电脑，必定有 CPU、主板、显示器等部件。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： // 得到 Intel 的 CPU CPUFactory cpuFactory = new IntelCPUFactory(); CPU cpu = intelCPUFactory.makeCPU(); // 得到 AMD 的主板 MainBoardFactory mainBoardFactory = new AmdMainBoardFactory(); MainBoard mainBoard = mainBoardFactory.make(); // 组装 CPU 和主板 Computer computer = new Computer(cpu, mainBoard); 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂方法模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，现实生活中存在一个问题，Intel 和 AMD 两家厂商有着商业竞争关系，它们就不想彼此的主板能相互使用。那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 因此这个时候，我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 public static void main(String[] args) { // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk); } 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式 单例模式的实现方式有很多种，每一种都有自己的特点。 饿汉式： public class Singleton { // 首先，将 new Singleton() 堵死 private Singleton() {}; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 private static Singleton instance = new Singleton(); public static Singleton getInstance() { return instance; } // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) {return new Date();} } 看到网上很多人说饿汉式的缺点：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。实际上本人觉得这都是可以规避的。另外，如果不想使用它，为什么要写这行代码呢？ 懒汉式： public class Singleton { // 首先，也是先堵死 new Singleton() 这条路 private Singleton() {} // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; // 这个地方需要加锁，当然有两个加锁的地方 // 第一种： public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } // 第二种： public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 当然，synchronized 第二种比第一种性能差一点，毕竟每次获取时都要获取锁。 最后，枚举类天生为单例模式，用 JVM 保证为单例。 建造模式 经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： Food food = new FoodBuilder().a().b().c().build(); Food food = Food.builder().a().b().c().build(); 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： class User { // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) { this.name = name; this.password = password; this.nickName = nickName; this.age = age; } // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() { return new UserBuilder(); } public static class UserBuilder { // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() { } // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) { this.name = name; return this; } public UserBuilder password(String password) { this.password = password; return this; } public UserBuilder nickName(String nickName) { this.nickName = nickName; return this; } public UserBuilder age(int age) { this.age = age; return this; } // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() { if (name == null || password == null) { throw new RuntimeException(\"用户名和密码必填\"); } if (age = 150) { throw new RuntimeException(\"年龄不合法\"); } // 还可以做赋予”默认值“的功能 if (nickName == null) { nickName = name; } return new User(name, password, nickName, age); } } } 核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： public class APP { public static void main(String[] args) { User d = User.builder() .name(\"foo\") .password(\"pAss12345\") .age(25) .build(); } } 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: @Builder class User { private String name; private String password; private String nickName; private int age; } 怎么样，省下来的时间是不是又可以干点别的了。 当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： User user = new User().setName(\"\").setPassword(\"\").setAge(20); 原型模式 原型模式对比到现实生活中就是克隆人差不多的概念。不光克隆外表，连内在都一模一样。 在 Java 中有浅拷贝和深拷贝两种，其实，原型模式的实现之一就是深拷贝。即基于现有的对象复制出来一个一样的对象。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 protected native Object clone() throws CloneNotSupportedException; 原型模式就说这么多了，网上有很多变种说原型模型，个人觉得其实本质上就是根据已有的对象复制出来一个地址不同的对象。 结构型模式 前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式 个人觉得代理模式是必须需要掌握的。代理模式可以帮助我们屏蔽底层实现类的实现细节，在实现前后添加一部分逻辑。即通过代理对象访问目标对象，这样可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。 其实，理解了“代理”这个词的意思，其实这个模式也就理解了。 public interface FoodService { Food makeChicken(); Food makeNoodle(); } public class FoodServiceImpl implements FoodService { public Food makeChicken() { Food f = new Chicken() f.setChicken(\"1kg\"); f.setSpicy(\"1g\"); f.setSalt(\"3g\"); return f; } public Food makeNoodle() { Food f = new Noodle(); f.setNoodle(\"500g\"); f.setSalt(\"5g\"); return f; } } // 代理要表现得“就像是”真实实现类，所以需要实现 FoodService public class FoodServiceProxy implements FoodService { // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() { System.out.println(\"我们马上要开始制作鸡肉了\"); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(\"鸡肉制作完成啦，加点胡椒粉\"); // 增强 food.addCondiment(\"pepper\"); return food; } public Food makeNoodle() { System.out.println(\"准备制作拉面~\"); Food food = foodService.makeNoodle(); System.out.println(\"制作完成啦\") return food; } } 客户端调用，注意，我们要用代理来实例化接口： // 这里用代理类来实例化 FoodService foodService = new FoodServiceProxy(); foodService.makeChicken(); 代理模式本质上就是对既有方法不改动的情况下，对方法进行“方法增强”。在 SPRING 框架中，面向切面这一概念就是基于代理模式。在 spring 中我们不需要自己定义代理类，框架会帮我们动态代理我们的类。在spring 中的动态代理，又分为 JDK 动态代理（面向接口） 和 CGlib 代理。感兴趣的可以去网上查阅这部分的资料。 享元模式 享元模式我更喜欢自己给他起的名字——缓存模式，即通过容器将创建好的对象缓存起来，之后每次用，就直接从容器中取出来就好了，不需要二次创建。在 java 的包装类中就能看见这样的代码（想必大家面试的时候可能被问过 Integer 是否相等的问题）： public static Integer valueOf(int i) { if (i >= IntegerCache.low && i 享元模式就讲到这了。 组合模式 其实组合模式最常见的实现，就是树这种数据结构了。组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 上面的话就是，整体对象的每一个子节点有着相同的访问方式和使用方式。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 public class Employee { private String name; private String dept; private int salary; private List subordinates; // 下属 public Employee(String name,String dept, int sal) { this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList(); } public void add(Employee e) { subordinates.add(e); } public void remove(Employee e) { subordinates.remove(e); } public List getSubordinates(){ return subordinates; } public String toString(){ return (\"Employee :[ Name : \" + name + \", dept : \" + dept + \", salary :\" + salary+\" ]\"); } } 门面模式 门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： public interface Shape { void draw(); } 定义几个实现类： public class Circle implements Shape { @Override public void draw() { System.out.println(\"Circle::draw()\"); } } public class Rectangle implements Shape { @Override public void draw() { System.out.println(\"Rectangle::draw()\"); } } 客户端调用： public static void main(String[] args) { // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw(); } 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： public class ShapeMaker { private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() { circle = new Circle(); rectangle = new Rectangle(); square = new Square(); } /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle(){ circle.draw(); } public void drawRectangle(){ rectangle.draw(); } public void drawSquare(){ square.draw(); } } 看看现在客户端怎么调用： public static void main(String[] args) { ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); } 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 门面模式（未完成） 桥梁模式（未完成） 适配器模式（未完成） 这里就不给大家说了，是在分不清和代理模式有什么区别，这里就不给大家误导了。 行为型模式 行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式 策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 首先，先定义一个策略接口： public interface Strategy { public void draw(int radius, int x, int y); } 然后我们定义具体的几个策略： public class RedPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(\"用红色笔画图，radius:\" + radius + \", x:\" + x + \", y:\" + y); } } public class GreenPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(\"用绿色笔画图，radius:\" + radius + \", x:\" + x + \", y:\" + y); } } public class BluePen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(\"用蓝色笔画图，radius:\" + radius + \", x:\" + x + \", y:\" + y); } } 使用策略的类： public class Context { private Strategy strategy; public Context(Strategy strategy){ this.strategy = strategy; } public int executeDraw(int radius, int x, int y){ return strategy.draw(radius, x, y); } } 客户端演示： public static void main(String[] args) { Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0); } 观察者模式 观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： public class Subject { private List observers = new ArrayList(); private int state; public int getState() { return state; } public void setState(int state) { this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); } // 注册观察者 public void attach(Observer observer) { observers.add(observer); } // 通知观察者们 public void notifyAllObservers() { for (Observer observer : observers) { observer.update(); } } } 定义观察者接口： public abstract class Observer { protected Subject subject; public abstract void update(); } 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： public class BinaryObserver extends Observer { // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) { this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); } // 该方法由主题类在数据变更的时候进行调用 @Override public void update() { String result = Integer.toBinaryString(subject.getState()); System.out.println(\"订阅的数据发生变化，新的数据处理为二进制值为：\" + result); } } public class HexaObserver extends Observer { public HexaObserver(Subject subject) { this.subject = subject; this.subject.attach(this); } @Override public void update() { String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println(\"订阅的数据发生变化，新的数据处理为十六进制值为：\" + result); } } 客户端使用也非常简单： public static void main(String[] args) { // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11); } output: 订阅的数据发生变化，新的数据处理为二进制值为：1011 订阅的数据发生变化，新的数据处理为十六进制值为：B 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 还有，即使是上面的这个代码，也会有很多变种，大家只要记住核心的部分，那就是一定有一个地方存放了所有的观察者，然后在事件发生的时候，遍历观察者，调用它们的回调函数。 责任链模式 责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： public abstract class RuleHandler { // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) { this.successor = successor; } public RuleHandler getSuccessor() { return successor; } } 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： public class NewUserRuleHandler extends RuleHandler { public void apply(Context context) { if (context.isNewUser()) { // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(\"该活动仅限新用户参与\"); } } } 校验用户所在地区是否可以参与： public class LocationRuleHandler extends RuleHandler { public void apply(Context context) { boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) { if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(\"非常抱歉，您所在的地区无法参与本次活动\"); } } } 校验奖品是否已领完： public class LimitRuleHandler extends RuleHandler { public void apply(Context context) { int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes > 0) { if (this.getSuccessor() != null) { this.getSuccessor().apply(userInfo); } } else { throw new RuntimeException(\"您来得太晚了，奖品被领完了\"); } } } 客户端： public static void main(String[] args) { RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context); } 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 至于它和我们前面说的用一个 List 存放需要执行的规则的做法有什么异同，留给读者自己琢磨吧。 模板方法模式 在含有继承结构的代码中，模板方法模式是非常常用的。 通常会有一个抽象类： public abstract class AbstractTemplate { // 这就是模板方法 public void templateMethod() { init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 } protected void init() { System.out.println(\"init 抽象层已经实现，子类也可以选择覆写\"); } // 留给子类实现 protected abstract void apply(); protected void end() { } } 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： public class ConcreteTemplate extends AbstractTemplate { public void apply() { System.out.println(\"子类实现抽象方法 apply\"); } public void end() { System.out.println(\"我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了\"); } } 客户端调用演示： public static void main(String[] args) { AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod(); } 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式 废话我就不说了，我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。 定义状态接口： public interface State { public void doAction(Context context); } 定义减库存的状态： public class DeductState implements State { public void doAction(Context context) { System.out.println(\"商品卖出，准备减库存\"); context.setState(this); //... 执行减库存的具体操作 } public String toString() { return \"Deduct State\"; } } 定义补库存状态： public class RevertState implements State { public void doAction(Context context) { System.out.println(\"给此商品补库存\"); context.setState(this); //... 执行加库存的具体操作 } public String toString() { return \"Revert State\"; } } 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： public class Context { private State state; private String name; public Context(String name) { this.name = name; } public void setState(State state) { this.state = state; } public void getState() { return this.state; } } 我们来看下客户端调用，大家就一清二楚了： public static void main(String[] args) { // 我们需要操作的是 iPhone X Context context = new Context(\"iPhone X\"); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString(); } 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 总结 学习设计模式的目的是为了让我们的代码更加的优雅、易维护、易扩展。其实觉得一篇文章写下来收货最大的是自己。哈哈，我往大家有空也可以参考着网上文档，结合着自己的经验写一下，收获满满。 "},"doc/design/status_machine.html":{"url":"doc/design/status_machine.html","title":"状态机模式","keywords":"","body":"背景 在需求开发的过程中，经常会遇到根据不同的情况作出不同的处理。最直接的就是if…else…。 当场景特别复杂时，判断if就有些力不从心了。加一个场景需要修改大量的代码，这不是一个很好的做法。程序的扩展性特别薄弱,代码的后期维护也是个大坑。 有限状态机 有限状态机（英语：finite-state machine，缩写：FSM）又称有限状态自动机，简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。 站在有限状态机的角度来看，可以抽象如下几个关键点： 状态（State） 当前活动所对应的状态。 事件（Event） 即在触发了某个事件才往下更改状态。 动作（Transition） 即状态流转过程中具体的业务逻辑。 订单状态流转图 订单状态流转图主要表达了事件、动作和状态之间的关系。事件有：提交订单、取消订单、派单、签收等。动作是流转过程中的具体业务逻辑，比如在订单提交时会根据支付类型选择订单可能的状态。上图中绿色方块代表的就是订单状态。 抽象设计 代码实现 1、事件枚举类：定义触发订单状态流转事件。 package com.pinpianyi.demo.order.state.constant; import lombok.Getter; /** * 订单事件枚举类 */ @Getter public enum OrderEvent { SUBMIT(10, \"提交订单\"), CANCEL(11, \"取消订单\"), COMPLETE_PAYMENT(12, \"支付完成\"), CHANGE_ORDER_TYPE(13, \"修改订单类型\"), APPOINTED_ORDER(14, \"派单\"), SIGN_AFTER_RECEIVING(15, \"签收\"); private String eventDesc; private int eventId; OrderEvent(int eventId, String desc) { this.eventDesc = desc; this.eventId = eventId; } } 2、订单状态枚举类：定义订单状态。 package com.pinpianyi.demo.order.state.constant; public enum OrderStatus { /** * 未提交 */ UN_COMMIT(0, \"未提交\"), /** * 等待支付 */ WAIT_PAY(100, \"待支付\"), /** * 在线支付支付完成 * 货到付款下单 */ WAIT_DISPATCH(200, \"待派单/待发货\"), /** * 任一子单已派单，父单置为配送中 */ DELIVERING(210, \"配送中\"), /** * 全部子单已完成或已关闭，且至少有一单已完成 */ COMPLETED(290, \"已完成\"), /** * 未知状态 */ UN_KNOW(444, \"未知\"), /** * 子单全部取消 */ CANCELED(295, \"已取消\"); OrderStatus(int status, String desc) { this.status = status; this.desc = desc; } public int status; public String desc; public static OrderStatus of(Integer statusCode) { for (OrderStatus status : values()) { if (status.status == statusCode) { return status; } } return null; } } 3、支付类型枚举类：定义支付类型。 package com.pinpianyi.demo.order.state.constant; import lombok.Getter; @Getter public enum OrderPayType { ONLINE_PAYMENT(0, \"在线支付\"), CASH_ON_DELIVERY(1, \"货到付款\"); private int type; private String desc; OrderPayType(int value, String desc) { this.type = value; this.desc = desc; } } 4、定义注解StatusMigration用于初始化时标识某个类用于处理订单状态流转。 package com.pinpianyi.demo.order.state.core.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; /** * 订单状态流传处理器，添加此注解用于配置初始化。 */ @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.TYPE}) public @interface StatusMigration { } 5、定义注解AfterProcessor用于初始化时标识某个类用于处理状态流转后的业务逻辑。 package com.pinpianyi.demo.order.state.core.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; /** * 订单状态流传操作后处理器标识注解，用于初始化操作 **/ @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.TYPE}) public @interface AfterProcessor { } 6、定义抽象类：AbstractOrderStatusMigration、OrderStatusChangedProcessor在初始化操作中完成对应处理器的初始化。 package com.pinpianyi.demo.order.state.core; import com.pinpianyi.demo.order.state.constant.OrderEvent; import com.pinpianyi.demo.order.state.constant.OrderPayType; import lombok.Data; @Data public abstract class AbstractOrderStatusMigration { private int eventId; /** * 订单状态流转接口 * * @param orderStatus 当前订单状态 * @param orderPayType 支付类型 * @param event 事件 * @return 流转后的订单状态 */ public abstract int handleEvent(int orderStatus, OrderPayType orderPayType, OrderEvent event); } package com.pinpianyi.demo.order.state.core; import lombok.Data; /** * 订单状态流转后处理接口 **/ @Data public abstract class OrderStatusChangedProcessor { private int status; public abstract boolean process(String orderId, Object... params); } 7、状态机的核心组件：OrderStatusMigrationManager调用状态流转接口和流转后的业务逻辑接口。 @Component @Slf4j public class OrderStatusMigrationManager { final Map orderOperatorMaps = new ConcurrentHashMap(); final Map orderProcessorMaps = new ConcurrentHashMap(); public OrderStatusMigrationManager() { } /** * 状态流转方法 * * @param orderId 订单id * @param event 流转的订单操作事件 * @param status 当前订单状态 * @return 流转后的订单状态 */ public int handleEvent(final String orderId, OrderPayType payType, OrderEvent event, final int status) { if (this.isFinalStatus(status)) { throw new IllegalArgumentException(\"handle event can't process final state order.\"); } log.info(\">>>>开始流转orderId={} 的订单状态。>>>订单状态流转结束。>>>开始处理状态流转后的业务逻辑。>>>业务处理完成！ entry : orderOperatorMaps.entrySet()) { if (event.getEventId() == entry.getKey()) { operator = entry.getValue(); } } if (null == operator) { throw new IllegalArgumentException(String.format(\"can't find proper operator. The parameter state :%s\", event.toString())); } return operator; } /** * 根据入参状态枚举实例获取对应的状态后处理器 * * @param status event * @return 状态变更后的业务逻辑处理器 */ private OrderStatusChangedProcessor getStatusChangedProcessor(OrderStatus status) { OrderStatusChangedProcessor processor = null; for (Map.Entry entry : orderProcessorMaps.entrySet()) { if (status.status == entry.getKey()) { processor = entry.getValue(); } } if (null == processor) { throw new IllegalArgumentException(String.format(\"can't find proper processor. The parameter state :%s\", status.toString())); } return processor; } /** * 判断订单是否可流转 * * @param status 订单状态码 * @return */ private boolean isFinalStatus(int status) { return OrderStatus.COMPLETED.status == status || OrderStatus.UN_KNOW.status == status || OrderStatus.CANCELED.status == status; } } 8、实现BeanPostProcessor接口，将具体的状态流转实现类和流转后的处理器注册到OrderStatusMigrationManager中。 @Component public class Initialization implements BeanPostProcessor { @Resource OrderStatusMigrationManager manager; @Nullable public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof AbstractOrderStatusMigration && bean.getClass().isAnnotationPresent(StatusMigration.class)) { AbstractOrderStatusMigration orderOperator = (AbstractOrderStatusMigration) bean; manager.orderOperatorMaps.put(orderOperator.getEventId(), orderOperator); } if (bean instanceof OrderStatusChangedProcessor && bean.getClass().isAnnotationPresent(AfterProcessor.class)) { OrderStatusChangedProcessor orderProcessor = (OrderStatusChangedProcessor) bean; manager.orderProcessorMaps.put(orderProcessor.getStatus(), orderProcessor); } return bean; } } 至此，基于状态机的订单状态流转的核心逻辑已介绍完毕，下面按照之前的设计实现一个具体的创建订单业务用于测试。 测试验证 1、定义Order @Data @AllArgsConstructor @NoArgsConstructor public class Order { private String orderId; private int orderStatus; } 2、实现订单状态流转接口 @Component @StatusMigration @Slf4j public class CreateOrderStatusMigration extends AbstractOrderStatusMigration { public CreateOrderStatusMigration() { super.setEventId(OrderEvent.SUBMIT.getEventId()); } @Override public int handleEvent(int orderStatus, OrderPayType payType, OrderEvent event) { log.info(\"订单当前状态是：{}，订单支付类型是：{}，触发订单状态流转事件是：{}\", orderStatus, payType.getDesc(), event.getEventDesc()); if (orderStatus != OrderStatus.UN_COMMIT.status || event != OrderEvent.SUBMIT) { throw new IllegalArgumentException(String.format(\"create operation can't handle the status: %s\", orderStatus)); } OrderStatus status; switch (payType) { case ONLINE_PAYMENT: status = OrderStatus.WAIT_PAY; break; case CASH_ON_DELIVERY: status = OrderStatus.WAIT_DISPATCH; break; default: status = OrderStatus.UN_KNOW; } log.info(\"流转后的订单状态是：{}，状态码:{}\", status.desc, status.status); return status.status; } } 3、实现订单状态流转后的业务逻辑处理接口 @Component @AfterProcessor @Slf4j public class CreateOrderStatusChangedProcessor extends OrderStatusChangedProcessor { public CreateOrderStatusChangedProcessor() { setStatus(OrderStatus.WAIT_PAY.status); } @Override public boolean process(String orderId, Object... params) { log.info(\"订单号：{} 的订单进入创建后处理器...\", orderId); log.info(\"说明：创建/取消订单对应的数据库修改，mq发送等操作，可以在此处process方法中完成\"); return true; } } 4、增加一个SpringBeanConfig用于bean实列化 @ComponentScan @Configuration public class SpringBeanConfig { @Bean public Initialization initialization() { return new Initialization(); } @Bean public OrderStatusMigrationManager orderStatusMigrationManager() { return new OrderStatusMigrationManager(); } } 5、编写单元测试用列代码 @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = SpringBeanConfig.class) @Slf4j public class StateMachineTest { @Autowired private OrderStatusMigrationManager orderStatusMigrationManager; private Order order; @Before public void init() { order = new Order(\"20201214000001\", OrderStatus.UN_COMMIT.status); } @Test public void createOrder() { Assert.assertEquals(100, orderStatusMigrationManager.handleEvent(order.getOrderId(), OrderPayType.ONLINE_PAYMENT, OrderEvent.SUBMIT, order.getOrderStatus())); } } 运行createOrder方法可得到如下结果： 1148 [main] DEBUG org.springframework.test.context.support.AbstractDirtiesContextTestExecutionListener - Before test method: context [DefaultTestContext@25bbe1b6 testClass = StateMachineTest, testInstance = com.pinpianyi.demo.order.test.StateMachineTest@2d928643, testMethod = createOrder@StateMachineTest, testException = [null], mergedContextConfiguration = [MergedContextConfiguration@5702b3b1 testClass = StateMachineTest, locations = '{}', classes = '{class com.pinpianyi.demo.order.SpringBeanConfig}', contextInitializerClasses = '[]', activeProfiles = '{}', propertySourceLocations = '{}', propertySourceProperties = '{}', contextCustomizers = set[[empty]], contextLoader = 'org.springframework.test.context.support.DelegatingSmartContextLoader', parent = [null]], attributes = map[[empty]]], class annotated with @DirtiesContext [false] with mode [null], method annotated with @DirtiesContext [false] with mode [null]. 1151 [main] INFO com.pinpianyi.demo.order.state.core.OrderStatusMigrationManager - >>>>开始流转orderId=20201214000001 的订单状态。>>>订单状态流转结束。>>>开始处理状态流转后的业务逻辑。>>>业务处理完成！ 测试结果符合预期，如果要测试更多的状态流转，只需要实现相关状态流转接口和流转后的业务逻辑处理类就可以了。这种设计方式非常适合处理有限状态的流转，可以将状态流转、流转后的处理逻辑与其它业务逻辑分离。 "},"doc/java/one.html":{"url":"doc/java/one.html","title":"Java 基础知识点","keywords":"","body":"一、Java语言特点 1.面向对象（继承、封装、多态） 2.平台无关（JVM、JRE） 3.解释编译并存 4.支持多线程 二、JRE、JDK JDK 是 Java Development Kit 缩写，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。 JRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。 三、什么是字节码？Java采用字节码有什么好处？ 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以， Java 程序运行时相对来说还是高效的（不过，和 C++，Rust，Go 等语言还是有一定差距的），而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。 四、Java是一门解释与编译并存的语言 高级语言分为： 编译型 ：编译型语言 会通过编译器将源代码一次性翻译成可被该平台执行的机器码。一般情况下，编译语言的执行速度比较快，开发效率比较低。常见的编译性语言有 C、C++、Go、Rust 等等。 解释型 ：解释型语言会通过解释器一句一句的将代码解释（interpret）为机器代码后再执行。解释型语言开发效率比较快，执行速度比较慢。常见的解释性语言有 Python、JavaScript、PHP 等等。 Java 语言既具有编译型语言的特征，也具有解释型语言的特征。因为 Java 程序要经过先编译，后解释两个步骤，由 Java 编写的程序需要先经过编译步骤，生成字节码（.class 文件），这种字节码必须由 Java 解释器来解释执行。 五、Oracle JDK vs OpenJDK 以下引用内容来自oracle官网 非常接近 - 我们的 Oracle JDK 版本构建过程基于 OpenJDK 7 构建，只添加了几个部分，例如部署代码，其中包括 Oracle 的 Java 插件和 Java WebStart 的实现，以及一些闭源的第三方组件，如图形光栅化器，一些开源的第三方组件，如 Rhino，以及一些零碎的东西，如附加文档或第三方字体。展望未来，我们的目的是开源 Oracle JDK 的所有部分，除了我们考虑商业功能的部分。 其实我们理解为，openJDK 源自于oracle的分支 六、Java与C++ 的区别 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理垃圾回收机制(GC)，不需要程序员手动释放无用内存。 C ++同时支持方法重载和操作符重载，但是 Java 只支持方法重载（操作符重载增加了复杂性，这与 Java 最初的设计思想不符）。 ...... 七、字符型常量和字符串常量的区别? 形式 : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。 含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。 占内存大小 ： 字符常量只占 2 个字节; 字符串常量占若干个字节。 八、Java 中的注释有三种 单行注释 多行注释 文档注释 九、标识符和关键字 标识符：在我们编写程序的时候，需要大量地为程序、类、变量、方法等取名字 关键字：有一些标识符，Java 语言已经赋予了其特殊的含义，只能用于特定的地方 十、Java关键字 访问控制：default、pubic、private、protected 基本类型：int、long、short、char、float、double、boolean 保留字 ：goto、const 包相关 ：import、 package ...... 十一、自增、自减 ++ 和 -- 运算符可以放在变量之前，也可以放在变量之后，当运算符放在变量之前时(前缀)，先自增/减，再赋值；当运算符放在变量之后时(后缀)，先赋值，再自增/减。 十二、continue、break 和 return 的区别是什么？ continue ：指跳出当前的这一次循环，继续下一次循环。 break ：指跳出整个循环体，继续执行循环下面的语句。 return; ：直接使用 return 结束方法执行，用于没有返回值函数的方法 return value; ：return 一个特定值，用于有返回值函数的方法 十三、静态方法为什么不能调用非静态成员? 静态方法是属于类的，在类加载的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，需要通过类的实例对象去访问。 在类的非静态成员不存在的时候静态成员就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。 十四、重载和重写的区别 重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理 重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法重载 发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。重写(两同两小一大) 重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。 方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写 十五、什么是可变长参数？ 从 Java5 开始，Java 支持定义可变长参数，所谓可变长参数就是允许在调用方法时传入不定长度的参数。当遇上方法重载时，优先匹配固定参数的方法 十六、Java 中有 8 种基本数据类型 6 种数字类型： 4 种整数型：byte、short、int、long 2 种浮点型：float、double 1 种字符类型：char 1 种布尔型：boolean 这八种基本类型都有对应的包装类分别为：Byte、Short、Integer、Long、Float、Double、Character、Boolean 。 十七、基本类型和包装类型的区别？ 成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null。 包装类型可用于泛型，而基本类型不可以。 基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。 相比于对象类型， 基本数据类型占用的空间非常小。 基本数据：局部变量存放在栈中，成员变量放在堆中 十八、包装类型的缓存机制了解么？ Java 基本数据类型的包装类型的大部分都用到了缓存机制来提升性能。 Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。 记住：所有整型包装类对象之间值的比较，全部使用 equals 方法比较。 十九、什么是自动拆装箱？ 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； "},"doc/java/base.html":{"url":"doc/java/base.html","title":"知识大纲","keywords":"","body":"一、Java文件结构（魔术、版本号、常量池等） 二、Java常见数据结构 list：ArrayList（数组） 、 LinkedList（链表） set：hashset 、LinkedHashSet、treeSet map： HashMap、ConcurrentHashMap 三、Java内存分布 程序计数器、虚拟机栈、本地方法栈、堆、方法区（JDK1.7之前的永久代、JDK1.8之后的元空间）、直接内存 线程自有：程序计数器、虚拟机栈、本地方法栈 四、Java内存模型（JMM） 在多线程代码中，哪些行为是正确的、合法的，以及多线程之间如何进行通信，代码中变量的读写行为如何反应到内存、CPU缓存的底层细节。 关键字：volatile、final和synchronized 五、对象分布、对象访问 对象分布：对象头、实例数据、对齐数据 对象访问：指针、句柄 六、Java对象创建过程 加载、分配内存（指针碰撞、空闲列表）、初始化零值、设置对象头、执行init 七、Java类加载过程 遇到如下情况会进行类加载： 遇到new 、static 等字节码指令 使用反射调用的时候时发现类未加载 初始化子类时发现父类未加载 执行main的方法 八、重载和重写 重载：类相同方法名的方法有多个 重写：子类对父类的方法覆盖 重写两同、两小、一大 加载顺序 加载（获取class文件的二进制流、在方法区生成元数据、在堆中生成class对象） 连接 验证 准备 解析 初始化 使用 卸载（堆类没有实例 、 没有任何地方引用 、类加载器被回收） 八、Java初始化顺序 父类静态字段、父类静态代码块、子类静态字段、子类静态代码块 、父类变量、父类代码块、父类构造函数、子类变量、子类代码块、子类构造函数 九、类加载器、双亲委派、破坏双亲委派 类加载器：BootstrapClassLoader 、ExtensionClassLoader 、AppClassLoader findClass ：不破坏 loadClass：破坏 GC 相关 十、OOM StackOverFlowError 、OutOfMemoryError 除了程序计数器以外，所有地方都会发生oom 十一、垃圾回收算法 GC算法：可达性算法、计数引用法 三大算法：标记-清除、标记-整理、标记-复制 ps：新生代常用复制算法（从各代垃圾特点说明） 十二、FULL GC system.gc class 无法加载到堆中 大对象等进入老年代不足 空间担保机制导致空间不足 CMS收集器导致空间不足 十二、Java访问权限修饰符 publish：所有都可以访问 private： protect：派生类、同包 default：派生类 十三、四种引用 强引用：new 出来的对象都是 软引用：缓存 弱引用：threadlocal 虚引用：对象回收 BQ 相关 十四、AQS Node：双向队列 mode: shard 、exclusive waitStatus：1 0 -1 -2 -3 prednode 、nextNode、waitNode head tail state: 大于0说明现在都有人占用锁 lock流程（公平锁）： 检查等待队列中是否存在等待线程 通过cas 获取锁 如果上述过程没有获取锁，检查是否是重入锁 清除队列中的取消状态 重复上面的流程 Condition （持有对象锁才能进行相应的操作） 常用类（ReentrantLock 、 CountDownLock、CyclicBarrier、Semaphore） 十五、BlockingQueue 插入：报错、阻塞、特殊值、超时 ArrayBlockingQueue：底层是数组，有界队列，如果我们要使用生产者-消费者模式，这是非常好的选择。 LinkedBlockingQueue：底层是链表，可以当做无界和有界队列来使用，所以大家不要以为它就是无界队列。 SynchronousQueue：本身不带有空间来存储任何元素，使用上可以选择公平模式和非公平模式。 PriorityBlockingQueue：是无界队列，基于数组，数据结构为二叉堆，数组第一个也是树的根节点总是最小值。 十六、atomic 原子类 、 atomic 集合类 、 atomic 引用类 io相关 十七、IO 类型 同步 or 异步、 阻塞 or 非阻塞 十八、IO 五种类型 同步阻塞 同步非阻塞 多路复用 信号 异步 十九、NIO三大组件 Buffer、channel 、selector 二十、netty（后面会写一篇基于源码的） bootstrap、group、channel、optional、handle、childhandle 线程池相关 二十一、线程池构造函数 coresize、maximumPoolSize、keepAliveTime、unit、workQueue、threadFactory、rejecthandler 二十二、线程池的五种状态 RUNNING、SHUTDOWN、STOP、TIDYING、TERMINATED 二十三、线程池excute 方法 int c = ctl.get(); if (workerCountOf(c) 二十四、线程的六种状态 new、ready、running、blocked、timeoutwait、waiting、Terminated 事务 ACID：原子性、一致性、隔离性、持久性 二十五、并发带来的四种事务问题 丢失修改 脏读 不可重复读 幻读 二十六、SQL事务隔离级别 读未提交 读已提交 可重复读 串行化 二十七、INNODB三类锁 意向锁是为了快速获取当前表是否加锁 记录锁 间隙锁 范围锁 二十八、MySQL 三种日志 binlog（主从同步） 、 redoLog（事务回滚）、undolog（奔溃恢复） 二十九、MyISAM vs InnoDB 行级锁支持 支持事务 崩溃恢复 三十、spring 5种隔离级别、7种传播行为 Redis相关 一、Redis内存碎片 申请内存时会多申请空间 清理垃圾时不是非常干净 二、Redis数据结构 string set list sortset （跳表） hash 三、Redis 备份方式 AOF、RDB 四、Redis 清理方式 定时删除 使用删除 五、Redis 删除过期数据六种策略 六、Redis 使用产生问题即解决方案 问题：缓存穿透、缓存雪崩 方案：1、提高前置拦截检查、2、缓存随机过期 七、缓存一致性问题 结论：先更新数据库然后删除缓存 why：从数据库和缓存执行先后顺序说明 消息队列 一、作用 异步 削峰 解耦 二、副作用 降低系统可用性 提供系统的复杂性 一致性问题 三、常见的消息队列 Redis kafka rocketMq rabbitMq 四、RocketMQ 组件 product、broke-master、broke-slave、nameserver、comsumer 五、消息队列常见问题 重复消费 消息堆积 顺序消费 消息丢失 六、事务消息 基于2PC 协议 七、RocketMQ为什么快 网络通信：使用netty 储存：fileChannel、buffer、commitLog 1Gb 消费：consumerlog 顺序读等 网络 一、IOS 七层、TCP 四层 七层：应用层、表示层、会话层、传输层、网络层、链路层、物理层 四层：应用层、传输层、网络层、物理层 为什么要分层： 各层之间相互独立 提高了整体灵活性 大问题化小 二、HTTP VS HTTPS 三、HTTP1.0 VS HTTP1.1 连接方式：1.0短连接 、 1.1 长连接 新增24中响应码 缓存处理 加入host请求头 四、tcp 三次握手 、四次挥手 三次握手：syn、syn/ack、ack 四次挥手：fin、ack、 fin 、ack 为什么会有三次握手、四次挥手？ 面向连接，建立可靠的信道 五、浏览器浏览网页全过程 首先浏览器输入网址，然后host、路由器、DNS缓存中解析目标网址ip 通过ip 建立tcp连接 发送HTTP请求 建立成功后目标服务器响应，最后生成响应报文 浏览器接受报文后渲染页面 六、tcp 如何保证可靠 重试重传 ack 机制 阻塞控制 流量控制 操作系统 一、什么是操作系统？ 计算器管理硬件和软件的基石 本质上是运行在计算器上的一套软件系统 屏蔽了硬件层的差异性和复杂性 管理着计算器上的文件系统、内存等 二、系统调用 计算器分为：用户态和系统态，当我们需要系统内存时，需要用户态向系统态发出操作指令，让系统态给我们申请，这一过程称之为系统调用 三、进程和线程 进程是资源分配的最小单位，线程是CPU调度的最小单位 四、死锁产生的条件 互斥 等待 非抢占 循环等待 五、操作系统分配内存 连续分配：进程预分配一段连续内存 非连续分配：逻辑内存和物理内存对应 六、虚拟内存 Linux：swap 空间 虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间 设计模式 三大类 创建类（5种） 结构类（7种） 行为类（11种） 中间件既框架 一、spring prepareRefresh : 加载主类，并设置beandefefiendRead refresh obtainFreshBeanFactory 销毁存在的beanFactory 创建新的defaultListBeanFactory 设置是否可以循环依赖、是否可以bean覆盖 loadBeanDefinitions 加载所有的bean 定义 prepareBeanFactory 设置一些需要特殊处理的bean 初始化一些bean postProcessBeanFactory 注册beanFactoryPostProcess invokeBeanFactoryPostProcessors （实现了自动装配） registerBeanPostProcessors 注册 beanPostProcess finishBeanFactoryInitialization 实例化所有non - lazy bean getBean -> doGetBean getSingleton 三级缓存 doCreateBean createBeanInstance populateBean initializeBean applyBeanPostProcessorsBeforeInitialization invokeInitMethods applyBeanPostProcessorsAfterInitialization 在这里实现了aop，利用cglib or jdk 代理实现的 "},"doc/java/three.html":{"url":"doc/java/three.html","title":"基础知识三","keywords":"","body":"一、Exception 和 Error 有什么区别？ 在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类: Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。 Error ：Error 属于程序无法处理的错误 ，我们没办法通过 catch 来进行捕获不建议通过catch捕获 。例如 Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。 二、序列化 or 反序列化 序列化： 将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流的过程转换成数据结构或者对象的过程 三、Checked Exception 和 Unchecked Exception 有什么区别？ Checked Exception 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws 关键字处理的话，就没办法通过编译。 Unchecked Exception 即 不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 四、finally 中的代码一定会执行吗？ 虚拟机被终止运行 程序所在的线程死亡 关闭 CPU 五、使用 try-with-resources 适用范围（资源的定义）： 任何实现 java.lang.AutoCloseable或者 java.io.Closeable 的对象 关闭资源和 finally 块的执行顺序： 在 try-with-resources 语句中，任何 catch 或 finally 块在声明的资源关闭后运行 六、异常使用有哪些需要注意的地方？ 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出NumberFormatException而不是其父类IllegalArgumentException。 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在一段代码逻辑中）。 七、什么是泛型？有什么作用？ Java 泛型（Generics） 是 JDK 5 中引入的一个新特性。使用泛型参数，可以增强代码的可读性以及稳定性。 八、泛型的使用方式有哪几种？ 泛型一般有三种使用方式:泛型类、泛型接口、泛型方法。 九、何为反射？ 赋予了我们在运行时分析类以及执行类中方法的能力。通过反射你可以获取任意一个类的所有属性和方法，你还可以调用这些方法和属性。 十、反射机制优缺点 优点 ： 可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利 缺点 ：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。 十一、什么是序列化?什么是反序列化? 序列化： 将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过程 十二、Java 序列化中如果有些字段不想进行序列化 对于不想进行序列化的变量，使用 transient 关键字修饰 十三、Java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 十四、既然有了字节流,为什么还要有字符流? 字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 "},"doc/io/":{"url":"doc/io/","title":"java io","keywords":"","body":"何为 I/O? I/O（Input/Outpu） 即输入／输出 。 我们先从计算机结构的角度来解读一下 I/O。 根据冯.诺依曼结构，计算机结构分为 5 大部分：运算器、控制器、存储器、输入设备、输出设备。 输入设备即向计算机（CPU）输入数据，输出设备接收计算机输出的数据。 我们常见的键盘、鼠标是输入设备，显示器是输出设备。 从计算机结构的视角来看的话， I/O 描述了计算机系统与外部设备之间通信的过程。 我们再先从应用程序的角度来解读一下 I/O。 为了保证操作系统的稳定性和安全性，一个进程的地址空间划分为 用户空间（User space） 和 内核空间（Kernel space ） 。 像我们平常运行的应用程序都是运行在用户空间，只有内核空间才能进行系统态级别的资源有关的操作，比如如文件管理、进程通信、内存管理等等。也就是说，我们想要进行 IO 操作，一定是要依赖内核空间的能力。 并且，用户空间的程序不能直接访问内核空间。 当想要执行 IO 操作时，由于没有执行这些操作的权限，只能发起系统调用请求操作系统帮忙完成。 因此，用户进程想要执行 IO 操作的话，必须通过 系统调用 来间接访问内核空间 我们在平常开发过程中接触最多的就是 磁盘 IO（读写文件） 和 网络 IO（网络请求和相应）。 从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。 当应用程序发起 I/O 调用后，会经历两个步骤： 内核等待 I/O 设备准备好数据 内核将数据从内核空间拷贝到用户空间。 有哪些常见的 IO 模型? UNIX 系统下， IO 模型一共有 5 种： 同步阻塞 I/O、同步非阻塞 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。 这也是我们经常提到的 5 种 IO 模型。 Java 中 3 种常见 IO 模型 BIO (Blocking I/O) BIO 属于同步阻塞 IO 模型 。 同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到在内核把数据拷贝到用户空间。 在客户端连接数量不高的情况下，是没问题的。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (Non-blocking/New I/O) Java 中的 NIO 于 Java 1.4 中引入，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO 。 Java 中的 NIO 可以看作是 I/O 多路复用模型。也有很多人认为，Java 中的 NIO 属于同步非阻塞 IO 模型。 跟着我的思路往下看看，相信你会得到答案！ 我们先来看看 同步非阻塞 IO 模型。 同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。 相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。 但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 这个时候，I/O 多路复用模型 就上场了。 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间->用户空间）还是阻塞的。 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，是目前几乎在所有的操作系统上都有支持 select 调用 ：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用 ：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。 AIO (Asynchronous I/O) AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型。 异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。 最后，来一张图，简单总结一下 Java 中的 BIO、NIO、AIO。 参考 如何完成一次 IO：https://llc687.top/post/如何完成一次-io/ 程序员应该这样理解 IO：https://www.jianshu.com/p/fa7bdc4f3de7 10 分钟看懂， Java NIO 底层原理：https://www.cnblogs.com/crazymakercircle/p/10225159.html IO 模型知多少 | 理论篇：https://www.cnblogs.com/sheng-jie/p/how-much-you-know-about-io-models.html "},"doc/io/niodesc.html":{"url":"doc/io/niodesc.html","title":"java nio 三大组件 Buffer、Channel 和 Selector","keywords":"","body":"[toc] 本文将介绍 Java NIO 中三大组件 Buffer、Channel、Selector 的使用。 Buffer 一个 Buffer 本质上是内存中的一块，我们可以将数据写入这块内存，之后从这块内存获取数据。 java.nio 定义了以下几个 Buffer 的实现，这个图读者应该也在不少地方见过了吧。 其实核心是最后的 ByteBuffer，前面的一大串类只是包装了一下它而已，我们使用最多的通常也是 ByteBuffer。 我们应该将 Buffer 理解为一个数组，IntBuffer、CharBuffer、DoubleBuffer 等分别对应 int[]、char[]、double[] 等。 MappedByteBuffer 用于实现内存映射文件，也不是本文关注的重点。 我觉得操作 Buffer 和操作数组、类集差不多，只不过大部分时候我们都把它放到了 NIO 的场景里面来使用而已。下面介绍 Buffer 中的几个重要属性和几个重要方法。 position、limit、capacity 就像数组有数组容量，每次访问元素要指定下标，Buffer 中也有几个重要属性：position、limit、capacity。 最好理解的当然是 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值。 position 和 limit 是变化的，我们分别看下读和写操作下，它们是如何变化的。 position 的初始值是 0，每往 Buffer 中写入一个值，position 就自动加 1，代表下一次的写入位置。读操作的时候也是类似的，每读一个值，position 就自动加 1。 从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了。 Limit：写操作模式下，limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity。写结束后，切换到读模式，此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满了。 初始化 Buffer 每个 Buffer 实现类都提供了一个静态方法 allocate(int capacity) 帮助我们快速实例化一个 Buffer。如： ByteBuffer byteBuf = ByteBuffer.allocate(1024); IntBuffer intBuf = IntBuffer.allocate(1024); LongBuffer longBuf = LongBuffer.allocate(1024); // ... 另外，我们经常使用 wrap 方法来初始化一个 Buffer。 public static ByteBuffer wrap(byte[] array) { ... } 填充 Buffer 各个 Buffer 类都提供了一些 put 方法用于将数据填充到 Buffer 中，如 ByteBuffer 中的几个 put 方法： // 填充一个 byte 值 public abstract ByteBuffer put(byte b); // 在指定位置填充一个 int 值 public abstract ByteBuffer put(int index, byte b); // 将一个数组中的值填充进去 public final ByteBuffer put(byte[] src) {...} public ByteBuffer put(byte[] src, int offset, int length) {...} 上述这些方法需要自己控制 Buffer 大小，不能超过 capacity，超过会抛 java.nio.BufferOverflowException 异常。 对于 Buffer 来说，另一个常见的操作中就是，我们要将来自 Channel 的数据填充到 Buffer 中，在系统层面上，这个操作我们称为读操作，因为数据是从外部（文件或网络等）读到内存中。 int num = channel.read(buf); 上述方法会返回从 Channel 中读入到 Buffer 的数据大小。 提取 Buffer 中的值 前面介绍了写操作，每写入一个值，position 的值都需要加 1，所以 position 最后会指向最后一次写入的位置的后面一个，如果 Buffer 写满了，那么 position 等于 capacity（position 从 0 开始）。 如果要读 Buffer 中的值，需要切换模式，从写入模式切换到读出模式。注意，通常在说 NIO 的读操作的时候，我们说的是从 Channel 中读数据到 Buffer 中，对应的是对 Buffer 的写入操作，初学者需要理清楚这个。 调用 Buffer 的 flip() 方法，可以从写入模式切换到读取模式。其实这个方法也就是设置了一下 position 和 limit 值罢了。 public final Buffer flip() { limit = position; // 将 limit 设置为实际写入的数据数量 position = 0; // 重置 position 为 0 mark = -1; // mark 之后再说 return this; } 对应写入操作的一系列 put 方法，读操作提供了一系列的 get 方法： // 根据 position 来获取数据 public abstract byte get(); // 获取指定位置的数据 public abstract byte get(int index); // 将 Buffer 中的数据写入到数组中 public ByteBuffer get(byte[] dst) 附一个经常使用的方法： new String(buffer.array()).trim(); 当然了，除了将数据从 Buffer 取出来使用，更常见的操作是将我们写入的数据传输到 Channel 中，如通过 FileChannel 将数据写入到文件中，通过 SocketChannel 将数据写入网络发送到远程机器等。对应的，这种操作，我们称之为写操作。 int num = channel.write(buf); mark() & reset() 除了 position、limit、capacity 这三个基本的属性外，还有一个常用的属性就是 mark。 mark 用于临时保存 position 的值，每次调用 mark() 方法都会将 mark 设值为当前的 position，便于后续需要的时候使用。 public final Buffer mark() { mark = position; return this; } 那到底什么时候用呢？考虑以下场景，我们在 position 为 5 的时候，先 mark() 一下，然后继续往下读，读到第 10 的时候，我想重新回到 position 为 5 的地方重新来一遍，那只要调一下 reset() 方法，position 就回到 5 了。 public final Buffer reset() { int m = mark; if (m rewind() & clear() & compact() rewind()：会重置 position 为 0，通常用于重新从头读写 Buffer。 public final Buffer rewind() { position = 0; mark = -1; return this; } clear()：有点重置 Buffer 的意思，相当于重新实例化了一样。 通常，我们会先填充 Buffer，然后从 Buffer 读取数据，之后我们再重新往里填充新的数据，我们一般在重新填充之前先调用 clear()。 public final Buffer clear() { position = 0; limit = capacity; mark = -1; return this; } compact()：和 clear() 一样的是，它们都是在准备往 Buffer 填充新的数据之前调用。 前面说的 clear() 方法会重置几个属性，但是我们要看到，clear() 方法并不会将 Buffer 中的数据清空，只不过后续的写入会覆盖掉原来的数据，也就相当于清空了数据了。 而 compact() 方法有点不一样，调用这个方法以后，会先处理还没有读取的数据，也就是 position 到 limit 之间的数据（还没有读过的数据），先将这些数据移到左边，然后在这个基础上再开始写入。很明显，此时 limit 还是等于 capacity，position 指向原来数据的右边。 Channel 所有的 NIO 操作始于通道，通道是数据来源或数据写入的目的地，主要地，我们将关心 java.nio 包中实现的以下几个 Channel： FileChannel：文件通道，用于文件的读和写 DatagramChannel：用于 UDP 连接的接收和发送 SocketChannel：把它理解为 TCP 连接通道，简单理解就是 TCP 客户端 ServerSocketChannel：TCP 对应的服务端，用于监听某个端口进来的请求 FileChannel 我想文件操作对于大家来说应该是最熟悉的，不过我们在说 NIO 的时候，其实 FileChannel 并不是关注的重点。而且后面我们说非阻塞的时候会看到，FileChannel 是不支持非阻塞的。 这里算是简单介绍下常用的操作吧，感兴趣的读者瞄一眼就是了。 初始化： FileInputStream inputStream = new FileInputStream(new File(\"/data.txt\")); FileChannel fileChannel = inputStream.getChannel(); 当然了，我们也可以从 RandomAccessFile#getChannel 来得到 FileChannel。 读取文件内容： ByteBuffer buffer = ByteBuffer.allocate(1024); int num = fileChannel.read(buffer); 前面我们也说了，所有的 Channel 都是和 Buffer 打交道的。 写入文件内容： ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(\"随机写入一些内容到 Buffer 中\".getBytes()); // Buffer 切换为读模式 buffer.flip(); while(buffer.hasRemaining()) { // 将 Buffer 中的内容写入文件 fileChannel.write(buffer); } SocketChannel 我们前面说了，我们可以将 SocketChannel 理解成一个 TCP 客户端。虽然这么理解有点狭隘，因为我们在介绍 ServerSocketChannel 的时候会看到另一种使用方式。 打开一个 TCP 连接： SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(\"https://www.javadoop.com\", 80)); 当然了，上面的这行代码等价于下面的两行： // 打开一个通道 SocketChannel socketChannel = SocketChannel.open(); // 发起连接 socketChannel.connect(new InetSocketAddress(\"https://www.javadoop.com\", 80)); SocketChannel 的读写和 FileChannel 没什么区别，就是操作缓冲区。 // 读取数据 socketChannel.read(buffer); // 写入数据到网络连接中 while(buffer.hasRemaining()) { socketChannel.write(buffer); } 不要在这里停留太久，先继续往下走。 ServerSocketChannel 之前说 SocketChannel 是 TCP 客户端，这里说的 ServerSocketChannel 就是对应的服务端。 ServerSocketChannel 用于监听机器端口，管理从这个端口进来的 TCP 连接。 // 实例化 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 监听 8080 端口 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); while (true) { // 一旦有一个 TCP 连接进来，就对应创建一个 SocketChannel 进行处理 SocketChannel socketChannel = serverSocketChannel.accept(); } 这里我们可以看到 SocketChannel 的第二个实例化方式 到这里，我们应该能理解 SocketChannel 了，它不仅仅是 TCP 客户端，它代表的是一个网络通道，可读可写。 ServerSocketChannel 不和 Buffer 打交道了，因为它并不实际处理数据，它一旦接收到请求后，实例化 SocketChannel，之后在这个连接通道上的数据传递它就不管了，因为它需要继续监听端口，等待下一个连接。 DatagramChannel UDP 和 TCP 不一样，DatagramChannel 一个类处理了服务端和客户端。 科普一下，UDP 是面向无连接的，不需要和对方握手，不需要通知对方，就可以直接将数据包投出去，至于能不能送达，它是不知道的 监听端口： DatagramChannel channel = DatagramChannel.open(); channel.socket().bind(new InetSocketAddress(9090)); ByteBuffer buf = ByteBuffer.allocate(48); channel.receive(buf); 发送数据： String newData = \"New String to write to file...\" + System.currentTimeMillis(); ByteBuffer buf = ByteBuffer.allocate(48); buf.put(newData.getBytes()); buf.flip(); int bytesSent = channel.send(buf, new InetSocketAddress(\"jenkov.com\", 80)); Selector NIO 三大组件就剩 Selector 了，Selector 建立在非阻塞的基础之上，大家经常听到的 多路复用 在 Java 世界中指的就是它，用于实现一个线程管理多个 Channel。 读者在这一节不能消化 Selector 也没关系，因为后续在介绍非阻塞 IO 的时候还得说到这个，这里先介绍一些基本的接口操作。 首先，我们开启一个 Selector。你们爱翻译成选择器也好，多路复用器也好。 Selector selector = Selector.open(); 将 Channel 注册到 Selector 上。前面我们说了，Selector 建立在非阻塞模式之上，所以注册到 Selector 的 Channel 必须要支持非阻塞模式，FileChannel 不支持非阻塞，我们这里讨论最常见的 SocketChannel 和 ServerSocketChannel。 // 将通道设置为非阻塞模式，因为默认都是阻塞模式的 channel.configureBlocking(false); // 注册 SelectionKey key = channel.register(selector, SelectionKey.OP_READ); register 方法的第二个 int 型参数（使用二进制的标记位）用于表明需要监听哪些感兴趣的事件，共以下四种事件： SelectionKey.OP_READ 对应 00000001，通道中有数据可以进行读取 SelectionKey.OP_WRITE 对应 00000100，可以往通道中写入数据 SelectionKey.OP_CONNECT 对应 00001000，成功建立 TCP 连接 SelectionKey.OP_ACCEPT 对应 00010000，接受 TCP 连接 我们可以同时监听一个 Channel 中的发生的多个事件，比如我们要监听 ACCEPT 和 READ 事件，那么指定参数为二进制的 00010001 即十进制数值 17 即可。 注册方法返回值是 SelectionKey 实例，它包含了 Channel 和 Selector 信息，也包括了一个叫做 Interest Set 的信息，即我们设置的我们感兴趣的正在监听的事件集合。 调用 select() 方法获取通道信息。用于判断是否有我们感兴趣的事件已经发生了。 Selector 的操作就是以上 3 步，这里来一个简单的示例，大家看一下就好了。之后在介绍非阻塞 IO 的时候，会演示一份可执行的示例代码。 Selector selector = Selector.open(); channel.configureBlocking(false); SelectionKey key = channel.register(selector, SelectionKey.OP_READ); while(true) { // 判断是否有事件准备好 int readyChannels = selector.select(); if(readyChannels == 0) continue; // 遍历 Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if(key.isAcceptable()) { // a connection was accepted by a ServerSocketChannel. } else if (key.isConnectable()) { // a connection was established with a remote server. } else if (key.isReadable()) { // a channel is ready for reading } else if (key.isWritable()) { // a channel is ready for writing } keyIterator.remove(); } } 对于 Selector，我们还需要非常熟悉以下几个方法： select() 调用此方法，会将上次 select 之后的准备好的 channel 对应的 SelectionKey 复制到 selected set 中。如果没有任何通道准备好，这个方法会阻塞，直到至少有一个通道准备好。 selectNow() 功能和 select 一样，区别在于如果没有准备好的通道，那么此方法会立即返回 0。 select(long timeout) 看了前面两个，这个应该很好理解了，如果没有通道准备好，此方法会等待一会 wakeup() 这个方法是用来唤醒等待在 select() 和 select(timeout) 上的线程的。如果 wakeup() 先被调用，此时没有线程在 select 上阻塞，那么之后的一个 select() 或 select(timeout) 会立即返回，而不会阻塞，当然，它只会作用一次。 小结 到此为止，介绍了 Buffer、Channel 和 Selector 的常见接口。 Buffer 和数组差不多，它有 position、limit、capacity 几个重要属性。put() 一下数据、flip() 切换到读模式、然后用 get() 获取数据、clear() 一下清空数据、重新回到 put() 写入数据。 Channel 基本上只和 Buffer 打交道，最重要的接口就是 channel.read(buffer) 和 channel.write(buffer)。 Selector 用于实现非阻塞 IO，这里仅仅介绍接口使用，后续请关注非阻塞 IO 的介绍。 （全文完） "},"doc/container/":{"url":"doc/container/","title":"java 容器","keywords":"","body":" 1. Java 集合框架总结 1.1 集合概述 1.1.1 java 集合概览 1.1.2 List、Set、Map 数据结构总结 1.1.3 List、Set、Map 三者的区别 1.1.4 为什么要使用集合？ 1.1.5 怎么选择合适的集合？ 1.1.6 comparable 和 Comparator 的区别 1.2 collection 子接口 List 1.2.1 Arraylist 与 Vector 的区别 1.2.2 Arraylist 与 Linkedlist 的区别 1.3 collection 子接口 Set 1.3.1 无序性和不可重复性的含义是什么？ 1.3.2 比较 HashSet、LinkedHashSet、TreeSet 三者的异同 1.4 Map 接口 1.4.1 HashMap 和 HashTable 的区别 1.4.2 HashMap 和 HashSet 的区别 1.4.3 HashMap 和 TreeMap 的区别 1.5 Collections 工具类 1.5.1 排序操作 1.5.2 查找、替换操作 1. Java 集合框架总结 1.1 集合概述 1.1.1 java 集合概览 通过下图可以清晰的看出来，在 Java 中除了以 Map 结尾的类之外， 其他类都实现了 Collection 接口。 并且，以 Map 结尾的类都实现了 Map 接口。 1.1.2 List、Set、Map 数据结构总结 1.1.2.1 List Arraylist： Object[]数组 Vector：Object[]数组 LinkedList： 双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环) 1.1.2.2 Set HashSet（无序，唯一）: 基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet：LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的 LinkedHashMap 其内部是基于 HashMap 实现一样，不过还是有一点点区别的 TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树) 1.1.2.3 Map HashMap： JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。 Hashtable： 数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树） 1.1.3 List、Set、Map 三者的区别 List(对付顺序的好帮手)： 存储的元素是有序的、可重复的。 Set(注重独一无二的性质): 存储的元素是无序的、不可重复的。 Map(用 Key 来搜索的专家): 使用键值对（key-value）存储，类似于数学上的函数 y=f(x)，“x”代表 key，\"y\"代表 value，Key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值。 1.1.4 为什么要使用集合？ 我们知道，在java中有数组的概念，数组可以用来存放一组数据。但是，数组是固定长度的，这样在使用的时候就会有很多的不方便，比如说资源的浪费。这个时候，我们就希望有一种可以动态改变大小的数组，那就是集合的作用了。Java 所有的集合类都位于 java.util 包下，提供了一个表示和操作对象集合的统一构架，包含大量集合接口，以及这些接口的实现类和操作它们的算法。 1.1.5 怎么选择合适的集合？ Java 为您提供了多种收集实现供您选择。 通常，您将始终为您的编程任务寻找性能最佳的集合，在大多数情况下为ArrayList，HashSet或HashMap。但是请注意，如果您需要某些特殊功能（例如排序或排序），则可能需要进行特殊的实现。 该 Java 集合教程不包括WeakHashMap等很少使用的类，因为它们是为非常特定或特殊任务设计的，因此在 99% 的情况下都不应该选择它们。 1.1.5.1 选择正确的 Java Map 接口 HashMap –如果迭代时项目的顺序对您不重要，请使用此实现。与TreeMap和LinkedHashMap相比，HashMap具有更好的性能。 TreeMap – 已排序和排序，但比HashMap慢。TreeMap根据其比较器具有键的升序 LinkedHashMap – 在插入过程中按键对项目排序 1.1.5.2 选择正确的 Java List 接口 ArrayList –插入期间对项目进行排序。与对LinkedLists的搜索操作相比，对ArrayLists的搜索操作更快 LinkedList – 已快速添加到列表的开头，并通过迭代从内部快速删除 1.1.5.3 选择正确的 Java Set 接口 HashSet – 如果迭代时项目的顺序对您不重要，请使用此实现。与TreeSet和LinkedHashSet相比，HashSet具有更好的性能 LinkedHashSet – 在插入过程中排序元素 TreeSet – 根据其比较器，按键的升序排序 1.1.6 comparable 和 Comparator 的区别 1、如果实现类没有实现Comparable接口，又想对两个类进行比较（或者实现类实现了Comparable接口，但是对compareTo方法内的比较算法不满意），那么可以实现Comparator接口，自定义一个比较器，写比较算法 2、实现Comparable接口的方式比实现Comparator接口的耦合性要强一些，如果要修改比较算法，要修改Comparable接口的实现类，而实现Comparator的类是在外部进行比较的，不需要对实现类有任何修改。从这个角度说，其实有些不太好，尤其在我们将实现类的.class文件打成一个.jar文件提供给开发者使用的时候。实际上实现Comparator接口的方式后面会写到就是一种典型的策略模式。 1.2 collection 子接口 List 1.2.1 Arraylist 与 Vector 的区别 ArrayList 是 List 的主要实现类，底层使用 Object[ ]存储，适用于频繁的查找工作，线程不安全 ； Vector 是 List 的古老实现类，底层使用Object[ ] 存储，线程安全的。 1.2.2 Arraylist 与 Linkedlist 的区别 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响： ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 LinkedList 采用链表存储，所以，如果是在头尾插入或者删除元素不受元素位置的影响（add(E e)、addFirst(E e)、addLast(E e)、removeFirst() 、 removeLast()），近似 O(1)，如果是要在指定位置 i 插入和删除元素的话（add(int index, E element)，remove(Object o)） 时间复杂度近似为 O(n) ，因为需要先移动到指定位置再插入。 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 1.3 collection 子接口 Set 1.3.1 无序性和不可重复性的含义是什么？ 1、什么是无序性？无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。 2、什么是不可重复性？不可重复性是指添加的元素按照 equals()判断时 ，返回 false，需要同时重写 equals()方法和 HashCode()方法。 1.3.2 比较 HashSet、LinkedHashSet、TreeSet 三者的异同 HashSet 是 Set 接口的主要实现类 ，HashSet 的底层是 HashMap，线程不安全的，可以存储 null 值； LinkedHashSet 是 HashSet 的子类，能够按照添加的顺序遍历； TreeSet 底层使用红黑树，能够按照添加元素的顺序进行遍历，排序的方式有自然排序和定制排序。 1.4 Map 接口 1.4.1 HashMap 和 HashTable 的区别 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的,因为 HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它； 对 Null key 和 Null value 的支持： HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；HashTable 不允许有 null 键和 null 值，否则会抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 1.4.2 HashMap 和 HashSet 的区别 如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap HashSet 实现了 Map 接口 实现 Set 接口 存储键值对 仅存储对象 调用 put()向 map 中添加元素 调用 add()方法向 Set 中添加元素 HashMap 使用键（Key）计算 hashcode HashSet 使用成员对象来计算 hashcode 值，对于两个对象来说 hashcode 可能相同，所以equals()方法用来判断对象的相等性 1.4.3 HashMap 和 TreeMap 的区别 TreeMap 和HashMap 都继承自AbstractMap ，但是需要注意的是TreeMap它还实现了NavigableMap接口和SortedMap 接口。 相比于HashMap来说 TreeMap 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。 1.5 Collections 工具类 1.5.1 排序操作 void reverse(List list)//反转 void shuffle(List list)//随机排序 void sort(List list)//按自然排序的升序排序 void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑 void swap(List list, int i , int j)//交换两个索引位置的元素 void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面 1.5.2 查找、替换操作 int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的 int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll) int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c) void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素 int frequency(Collection c, Object o)//统计元素出现次数 int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target) boolean replaceAll(List list, Object oldVal, Object newVal)//用新元素替换旧元素 "},"doc/container/arraylist.html":{"url":"doc/container/arraylist.html","title":"ArrayList","keywords":"","body":"1. ArrayList 简介 ArrayList 的底层是Object[]的数组队列，相当于动态数组。它对 Java 原生的数组进行了包装，相比较原生数组在创建时就指定了容量，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 ArrayList继承于 AbstractList ，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable{ } RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了 Cloneable 接口 ，即覆盖了函数clone()，能被克隆。 ArrayList 实现了 java.io.Serializable接口，这意味着ArrayList支持序列化，能通过序列化去传输。 1.1. Arraylist 和 Vector 的区别? ArrayList 是 List 的主要实现类，底层使用 Object[ ]存储，适用于频繁的查找工作，线程不安全 ； Vector 是 List 的古老实现类，底层使用 Object[ ]存储，线程安全的。 1.2. Arraylist 与 LinkedList 区别? 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 2. ArrayList 核心源码解读 package java.util; import java.util.function.Consumer; import java.util.function.Predicate; import java.util.function.UnaryOperator; public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = {}; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数（用户可以在创建ArrayList对象时自己指定集合的初始大小） */ public ArrayList(int initialCapacity) { if (initialCapacity > 0) { //如果传入的参数大于0，创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { //如果传入的参数等于0，创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else { //其他情况，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } /** *默认无参构造函数 *DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection c) { //将指定集合转换为数组 elementData = c.toArray(); //如果elementData数组的长度不为0 if ((size = elementData.length) != 0) { // 如果elementData不是Object类型数据（c.toArray可能返回的不是Object类型的数组所以加上下面的语句用于判断） if (elementData.getClass() != Object[].class) //将原来不是Object类型的elementData数组的内容，赋值给新的Object类型的elementData数组 elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // 其他情况，用空数组代替 this.elementData = EMPTY_ELEMENTDATA; } } /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() { modCount++; if (size minExpand) { ensureExplicitCapacity(minCapacity); } } //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取“默认的容量”和“传入参数”两者之间的最大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity >> 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) { if (minCapacity MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } /** *返回此列表中的元素数。 */ public int size() { return size; } /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() { //注意=和==的区别 return size == 0; } /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) { //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) >= 0; } /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) { if (o == null) { for (int i = 0; i = 0; i--) if (elementData[i]==null) return i; } else { for (int i = size-1; i >= 0; i--) if (o.equals(elementData[i])) return i; } return -1; } /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() { try { ArrayList v = (ArrayList) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); } } /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() { return Arrays.copyOf(elementData, size); } /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings(\"unchecked\") public T[] toArray(T[] a) { if (a.length size) a[size] = null; return a; } // Positional Access Operations @SuppressWarnings(\"unchecked\") E elementData(int index) { return (E) elementData[index]; } /** * 返回此列表中指定位置的元素。 */ public E get(int index) { rangeCheck(index); return elementData(index); } /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) { //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; } /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; } /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved > 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; } /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) { if (o == null) { for (int index = 0; index 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work } /** * 从列表中删除所有元素。 */ public void clear() { modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; } /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection c) { rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved > 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) { modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i = size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) { if (index > size || index c) { Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); } /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection c) { Objects.requireNonNull(c); return batchRemove(c, true); } /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator listIterator(int index) { if (index size) throw new IndexOutOfBoundsException(\"Index: \"+index); return new ListItr(index); } /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator listIterator() { return new ListItr(0); } /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator iterator() { return new Itr(); } 3. ArrayList 扩容机制分析 3.1. 先从 ArrayList 的构造函数说起 （JDK8）ArrayList 有三种方式来初始化，构造方法源码如下： /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) { if (initialCapacity > 0) {//初始容量大于0 //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } /** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; } } 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 下面在我们分析 ArrayList 扩容时会讲到这一点内容！ 补充：JDK6 new 无参构造的 ArrayList 对象时，直接创建了长度是 10 的 Object[] 数组 elementData 。 3.2. 一步一步分析 ArrayList 扩容机制 这里以无参构造函数创建的 ArrayList 为例分析 3.2.1. 先来看 add 方法 /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { //添加元素之前，先调用ensureCapacityInternal方法 ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; } 注意 ：JDK11 移除了 ensureCapacityInternal() 和 ensureExplicitCapacity() 方法 3.2.2. 再来看看 ensureCapacityInternal() 方法 （JDK7）可以看到 add 方法 首先调用了ensureCapacityInternal(size + 1) //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } 当 要 add 进第 1 个元素时，minCapacity 为 1，在 Math.max()方法比较后，minCapacity 为 10。 此处和后续 JDK8 代码格式化略有不同，核心代码基本一样。 3.2.3. ensureExplicitCapacity() 方法 如果调用 ensureCapacityInternal() 方法就一定会进入（执行）这个方法，下面我们来研究一下这个方法的源码！ //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length > 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } 我们来仔细分析一下： 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为 10。此时，minCapacity - elementData.length > 0成立，所以会进入 grow(minCapacity) 方法。 当 add 第 2 个元素时，minCapacity 为 2，此时 e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length > 0 不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。 直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。 3.2.4. grow() 方法 /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity >> 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } int newCapacity = oldCapacity + (oldCapacity >> 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如 ：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数. \">>\"（移位运算符）：>>1 右移一位相当于除 2，右移 n 位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了 1 位所以相当于 oldCapacity /2。对于大数据的 2 进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 我们再来通过例子探究一下grow() 方法 ： 当 add 第 1 个元素时，oldCapacity 为 0，经比较后第一个 if 判断成立，newCapacity = minCapacity(为 10)。但是第二个 if 判断不会成立，即 newCapacity 不比 MAX_ARRAY_SIZE 大，则不会进入 hugeCapacity 方法。数组容量为 10，add 方法中 return true,size 增为 1。 当 add 第 11 个元素进入 grow 方法时，newCapacity 为 15，比 minCapacity（为 11）大，第一个 if 判断不成立。新容量没有大于数组最大 size，不会进入 hugeCapacity 方法。数组容量扩为 15，add 方法中 return true,size 增为 11。 以此类推······ 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 3.2.5. hugeCapacity() 方法。 从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果 minCapacity 大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。 private static int hugeCapacity(int minCapacity) { if (minCapacity MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } 3.3. System.arraycopy() 和 Arrays.copyOf()方法 阅读源码的话，我们就会发现 ArrayList 中大量调用了这两个方法。比如：我们上面讲的扩容操作以及add(int index, E element)、toArray() 等方法中都用到了该方法！ 3.3.1. System.arraycopy() 方法 源码： // 我们发现 arraycopy 是一个 native 方法,接下来我们解释一下各个参数的具体意义 /** * 复制数组 * @param src 源数组 * @param srcPos 源数组中的起始位置 * @param dest 目标数组 * @param destPos 目标数组中的起始位置 * @param length 要复制的数组元素的数量 */ public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 场景： /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } 我们写一个简单的方法测试以下： public class ArraycopyTest { public static void main(String[] args) { // TODO Auto-generated method stub int[] a = new int[10]; a[0] = 0; a[1] = 1; a[2] = 2; a[3] = 3; System.arraycopy(a, 2, a, 3, 3); a[2]=99; for (int i = 0; i 结果： 0 1 99 2 3 0 0 0 0 0 3.3.2. Arrays.copyOf()方法 源码： public static int[] copyOf(int[] original, int newLength) { // 申请一个新的数组 int[] copy = new int[newLength]; // 调用System.arraycopy,将源数组中的数据进行拷贝,并返回新的数组 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } 场景： /** 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 */ public Object[] toArray() { //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); } 个人觉得使用 Arrays.copyOf()方法主要是为了给原有数组扩容，测试代码如下： public class ArrayscopyOfTest { public static void main(String[] args) { int[] a = new int[3]; a[0] = 0; a[1] = 1; a[2] = 2; int[] b = Arrays.copyOf(a, 10); System.out.println(\"b.length\"+b.length); } } 结果： 10 3.3.3. 两者联系和区别 联系： 看两者源代码可以发现 copyOf()内部实际调用了 System.arraycopy() 方法 区别： arraycopy() 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf() 是系统自动在内部新建一个数组，并返回该数组。 3.4. ensureCapacity方法 ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？ /** 如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity > minExpand) { ensureExplicitCapacity(minCapacity); } } 最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数 我们通过下面的代码实际测试以下这个方法的效果： public class EnsureCapacityTest { public static void main(String[] args) { ArrayList list = new ArrayList(); final int N = 10000000; long startTime = System.currentTimeMillis(); for (int i = 0; i 运行结果： 使用ensureCapacity方法前：2158 public class EnsureCapacityTest { public static void main(String[] args) { ArrayList list = new ArrayList(); final int N = 10000000; list = new ArrayList(); long startTime1 = System.currentTimeMillis(); list.ensureCapacity(N); for (int i = 0; i 运行结果： 使用ensureCapacity方法后：1773 通过运行结果，我们可以看出向 ArrayList 添加大量元素之前最好先使用ensureCapacity 方法，以减少增量重新分配的次数。 "},"doc/container/linkedlist.html":{"url":"doc/container/linkedlist.html","title":"LinkedList","keywords":"","body":" 1. 简介 2. 内部结构——双向链表 3. LinkedList 概述 3.1 LinkedList 构造方法 3.2 LinkedList 增删改查 3.3 LinkedList 遍历 1. 简介 LinkedList是一个实现了List接口和Deque接口的双端链表。 LinkedList底层的链表结构使它支持高效的插入和删除操作，另外它实现了Deque接口，使得LinkedList类也具有队列的特性; LinkedList不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法： List list=Collections.synchronizedList(new LinkedList(...)); 2. 内部结构——双向链表 看完了图之后，我们再看LinkedList类中的一个内部私有类Node就很好理解了： private static class Node { E item;//节点值 Node next;//后继节点 Node prev;//前驱节点 Node(Node prev, E element, Node next) { this.item = element; this.next = next; this.prev = prev; } } 这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。 3. LinkedList 概述 图中蓝色实线箭头是指继承关系 ，绿色虚线箭头是指接口实现关系。 1、LinkedList 继承自 AbstrackSequentialList 并实现了 List 接口以及 Deque 双向队列接口，因此 LinkedList 不但拥有 List 相关的操作方法，也有队列的相关操作方法。 2、LinkedList 和 ArrayList 一样实现了序列化接口 Serializable 和 Cloneable 接口使其拥有了序列化和克隆的特性。 LinkedList 一些主要特性： LinkedList 集合底层实现的数据结构为双向链表 LinkedList 集合中元素允许为 null LinkedList 允许存入重复的数据 LinkedList 中元素存放顺序为存入顺序。 LinkedList 是非线程安全的，如果想保证线程安全的前提下操作 LinkedList，可以使用 List list = Collections.synchronizedList(new LinkedList(...)); 来生成一个线程安全的 LinkedList 3.1 LinkedList 构造方法 LinkedList 有两个构造函数： /** * 空构造方法： */ public LinkedList() { } /** * 用已有的集合创建链表的构造方法： */ public LinkedList(Collection c) { this(); addAll(c); } 3.2 LinkedList 增删改查 新增 add(E e) 方法：将元素添加到链表尾部 public boolean add(E e) { linkLast(e);//这里就只调用了这一个方法 return true; } /** * 链接使e作为最后一个元素。 */ void linkLast(E e) { final Node l = last; final Node newNode = new Node<>(l, e, null); last = newNode;//新建节点 if (l == null) first = newNode; else l.next = newNode;//指向后继元素也就是指向下一个元素 size++; modCount++; } add(int index,E e)：在指定位置添加元素 public void add(int index, E element) { checkPositionIndex(index); //检查索引是否处于[0-size]之间 if (index == size)//添加在链表尾部 linkLast(element); else//添加在链表中间 linkBefore(element, node(index)); } linkBefore方法需要给定两个参数，一个插入节点的值，一个指定的node，所以我们又调用了Node(index)去找到index对应的node 查找 get(int index)： 根据指定索引返回数据 public E get(int index) { //检查index范围是否在size之内 checkElementIndex(index); //调用Node(index)去找到index对应的node然后返回它的值 return node(index).item; } 获取头节点（index=0）数据方法: public E getFirst() { final Node f = first; if (f == null) throw new NoSuchElementException(); return f.item; } public E element() { return getFirst(); } public E peek() { final Node f = first; return (f == null) ? null : f.item; } public E peekFirst() { final Node f = first; return (f == null) ? null : f.item; } 区别： getFirst(),element(),peek(),peekFirst() 这四个获取头结点方法的区别在于对链表为空时的处理，是抛出异常还是返回null，其中getFirst() 和element() 方法将会在链表为空时，抛出异常 element()方法的内部就是使用getFirst()实现的。它们会在链表为空时，抛出NoSuchElementException 获取尾节点（index=-1）数据方法: public E getLast() { final Node l = last; if (l == null) throw new NoSuchElementException(); return l.item; } public E peekLast() { final Node l = last; return (l == null) ? null : l.item; } 两者区别： getLast() 方法在链表为空时，会抛出NoSuchElementException，而peekLast() 则不会，只是会返回 null。 删除方法 remove() ,removeFirst(),pop(): 删除头节点 public E pop() { return removeFirst(); } public E remove() { return removeFirst(); } public E removeFirst() { final Node f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); } removeLast(),pollLast(): 删除尾节点 public E removeLast() { final Node l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); } public E pollLast() { final Node l = last; return (l == null) ? null : unlinkLast(l); } 区别： removeLast()在链表为空时将抛出NoSuchElementException，而pollLast()方法返回null。 remove(Object o): 删除指定元素 public boolean remove(Object o) { //如果删除对象为null if (o == null) { //从头开始遍历 for (Node x = first; x != null; x = x.next) { //找到元素 if (x.item == null) { //从链表中移除找到的元素 unlink(x); return true; } } } else { //从头开始遍历 for (Node x = first; x != null; x = x.next) { //找到元素 if (o.equals(x.item)) { //从链表中移除找到的元素 unlink(x); return true; } } } return false; } 当删除指定对象时，只需调用remove(Object o)即可，不过该方法一次只会删除一个匹配的对象，如果删除了匹配对象，返回true，否则false。 unlink(Node x) 方法： E unlink(Node x) { // assert x != null; final E element = x.item; final Node next = x.next;//得到后继节点 final Node prev = x.prev;//得到前驱节点 //删除前驱指针 if (prev == null) { first = next;//如果删除的节点是头节点,令头节点指向该节点的后继节点 } else { prev.next = next;//将前驱节点的后继节点指向后继节点 x.prev = null; } //删除后继指针 if (next == null) { last = prev;//如果删除的节点是尾节点,令尾节点指向该节点的前驱节点 } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } remove(int index)：删除指定位置的元素 public E remove(int index) { //检查index范围 checkElementIndex(index); //将节点删除 return unlink(node(index)); } 3.3 LinkedList 遍历 在 ArrayList 分析的时候，我们就知道 List 的实现类，有4中遍历方式：for 循环，高级 for 循环，Iterator 迭代器方法， ListIterator 迭代方法。由于 ArrayList 源码分析的时候比较详细看了源码，对于不同数据结构的 LinkedList 我们只看下他们的不同之处. LinkedList 没有单独 Iterator 实现类，它的 iterator 和 listIterator 方法均返回 ListItr的一个对象。 LinkedList 作为双向链表数据结构，过去上个元素和下个元素很方便。 下边我们来看下 ListItr 的源码： private class ListItr implements ListIterator { // 上一个遍历的节点 private Node lastReturned; // 下一次遍历返回的节点 private Node next; // cursor 指针下一次遍历返回的节点 private int nextIndex; // 期望的操作数 private int expectedModCount = modCount; // 根据参数 index 确定生成的迭代器 cursor 的位置 ListItr(int index) { // assert isPositionIndex(index); // 如果 index == size 则 next 为 null 否则寻找 index 位置的节点 next = (index == size) ? null : node(index); nextIndex = index; } // 判断指针是否还可以移动 public boolean hasNext() { return nextIndex 0; } // 当前游标位置的前一个元素 public E previous() { checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); // 等同于 lastReturned = next；next = (next == null) ? last : next.prev; // 发生在 index = size 时 lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; } public int nextIndex() { return nextIndex; } public int previousIndex() { return nextIndex - 1; } // 删除链表当前节点也就是调用 next/previous 返回的这节点，也就 lastReturned public void remove() { checkForComodification(); if (lastReturned == null) throw new IllegalStateException(); Node lastNext = lastReturned.next; //调用LinkedList 的删除节点的方法 unlink(lastReturned); if (next == lastReturned) next = lastNext; else nextIndex--; //上一次所操作的 节点置位空 lastReturned = null; expectedModCount++; } // 设置当前遍历的节点的值 public void set(E e) { if (lastReturned == null) throw new IllegalStateException(); checkForComodification(); lastReturned.item = e; } // 在 next 节点位置插入及节点 public void add(E e) { checkForComodification(); lastReturned = null; if (next == null) linkLast(e); else linkBefore(e, next); nextIndex++; expectedModCount++; } //简单哈操作数是否合法 final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } } "},"doc/container/hashmap.html":{"url":"doc/container/hashmap.html","title":"HashMap","keywords":"","body":" HashMap 简介 数据结构 源码分析 构造方法 put 操作分析 get 操作分析 resize 操作分析 总结 HashMap 简介 HashMap 不仅在我们日常开发中经常被使用，而且在很多框架中我们也能经常见到它的身影。那么作为一个开发者是有必要去了解它的实现机制的。 因为本人工作以来就没有使用过 JDK1.8 一下的版本，因此不对 JDK1.7 的HashMap进行分析。 HashMap 主要用来存放键值对，它基于哈希表的 Map 接口实现，是常用的 Java 集合之一，是非线程安全的。 HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个。 阅读前提：本文分析的是源码，所以至少读者要熟悉它们的接口使用，同时，对于并发，读者至少要知道 CAS、ReentrantLock、UNSAFE 操作这几个基本的知识，文中不会对这些知识进行介绍。Java8 用到了红黑树，不过本文不会进行展开，感兴趣的读者请自行查找相关资料。 数据结构 首先，我们用下面这张图来介绍 HashMap 的结构。 这个仅仅是示意图，因为没有考虑到数组要扩容的情况，具体的后面再说。 本质上，HashMap 是主体是个数组，每个 entry 又构成了一个单向的链表。 类的属性： public class HashMap extends AbstractMap implements Map, Cloneable, Serializable { // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 [] table; // 存放具体元素的集 transient Set> entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*加载因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor; } loadFactor 加载因子（稀疏因子） loadFactor 因子是控制数组存放数据的疏密程度。即当loadFactor 越趋近于 1，那么数组中存放的数据的饱和度就越大，同时 hash 冲突的概率越大，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。 loadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值。 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 threshold threshold = capacity * loadFactor，当 Size>=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 HashMap有两种数据节点：Node （普通节点）和 TreeNode（树型节点） Node 节点类源码: // 继承自 Map.Entry static class Node implements Map.Entry { final int hash;// 哈希值，当前节点的 hash 值 final K key;//键 V value;//值 // 指向下一个节点 Node next; Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } // override public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // override public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } TreeNode 节点类源码: static final class TreeNode extends LinkedHashMap.Entry { TreeNode parent; // 父 TreeNode left; // 左 TreeNode right; // 右 TreeNode prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node next) { super(hash, key, val, next); } // 返回根节点 final TreeNode root() { for (TreeNode r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } 源码分析 构造方法 HashMap 中有四个构造方法，它们分别如下： // 默认构造函数。 public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; } // 包含另一个“Map”的构造函数 public HashMap(Map m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//自己可以点进去看 } // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor put 操作分析 首先给大家看一下 put 流程图，大家看源码的时候根据这个流程图看，会轻松很多。 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } // 第四个参数 onlyIfAbsent 如果是 true，那么只有在不存在该 key 时才会进行 put 操作 // 第五个参数 evict 我们这里不关心 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; Node p; int n, i; // 第一次 put 值的时候，会触发下面的 resize()，类似 java7 的第一次 put 也要初始化数组长度 // 第一次 resize 和后续的扩容有些不一样，因为这次是数组从 null 初始化到默认的 16 或自定义的初始容量 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 找到具体的数组下标，如果此位置没有值，那么直接初始化一下 Node 并放置在这个位置就可以了 if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); else {// 数组该位置有数据 Node e; K k; // 首先，判断该位置的第一个数据和我们要插入的数据，key 是不是\"相等\"，如果是，取出这个节点 if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; // 如果该节点是代表红黑树的节点，调用红黑树的插值方法，本文不展开说红黑树 else if (p instanceof TreeNode) e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); else { // 到这里，说明数组该位置上是一个链表 for (int binCount = 0; ; ++binCount) { // 插入到链表的最后面(Java7 是插入到链表的最前面) if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // TREEIFY_THRESHOLD 为 8，所以，如果新插入的值是链表中的第 8 个 // 会触发下面的 treeifyBin，也就是将链表转换为红黑树 if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 如果在该链表中找到了\"相等\"的 key(== 或 equals) if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) // 此时 break，那么 e 为链表中[与要插入的新值的 key \"相等\"]的 node break; p = e; } } // e!=null 说明存在旧值的key与要插入的key\"相等\" // 对于我们分析的put操作，下面这个 if 其实就是进行 \"值覆盖\"，然后返回旧值 if (e != null) { V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 如果 HashMap 由于新插入这个值导致 size 已经超过了阈值，需要进行扩容 if (++size > threshold) resize(); afterNodeInsertion(evict); return null; } get 操作分析 相对于 put 来说，get 真的太简单了。 计算 key 的 hash 值，根据 hash 值找到对应数组下标: hash & (length-1) 判断数组该位置处的元素是否刚好就是我们要找的，如果不是，走第三步 判断该元素类型是否是 TreeNode，如果是，用红黑树的方法取数据，如果不是，走第四步 遍历链表，直到找到相等(==或equals)的 key public V get(Object key) { Node e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node getNode(int hash, Object key) { Node[] tab; Node first, e; int n; K k; if ((tab = table) != null && (n = tab.length) > 0 && (first = tab[(n - 1) & hash]) != null) { // 判断第一个节点是不是就是需要的 if (first.hash == hash && // always check first node ((k = first.key) == key || (key != null && key.equals(k)))) return first; if ((e = first.next) != null) { // 判断是否是红黑树 if (first instanceof TreeNode) return ((TreeNode)first).getTreeNode(hash, key); // 链表遍历 do { if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } resize 操作分析 进行扩容，会伴随着一次重新 hash 分配，并且会遍历 hash 表中所有的元素，是非常耗时的。在编写程序中，要尽量避免 resize。 final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold newCap = oldThr; else { // signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode)e).split(this, newTab, j, oldCap); else { Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; // 原索引 if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 总结 本文只是介绍了 HashMap 的源码，并没有给大家介绍线程安全的 ConcurrentHashMap 。以后有空会给大家写一期对ConcurrentHashMap 的讲解。 看源码不算是目的吧，深入地了解 Doug Lea 的设计思路，向大佬膜拜。 "},"doc/messagequeue/":{"url":"doc/messagequeue/","title":"消息队列","keywords":"","body":" 1. 消息队列简介 2. 为什么要使用消息队列 3. 使用消息队列我们需要关注的一些问题 4. 消息队列的两种协议 JMS 、 AMQP 5. 常见的消息队列对比 前言 为什么要写消息队列？主要和自己目前在公司负责 IM系统和交易系统有关。在其中很多地方聊天消息削峰、对交易链路的异步化，这些都是借助 MQ 的能力，在我们系统中得以完成。 1. 消息队列简介 我们日常生活中使用的快递，实际就是一个消息队列的模型。商家把快递邮寄到快递点，快递点通知我们有快递去签收。可能通知的时候我们正在上班，没有时间去取快递。等到我们下班的时候再去取快递。 消息队列作为分布式系统中不可缺失的一环，通过将同步改为异步处理的方式，提高了系统性能，将大流量削峰、降低系统耦合性。 通过使用 Queue 这种先进先出的数据结构，保证了消息消费时也是按照顺序消费的。 2. 为什么要使用消息队列? 回答这个问题前，我们要思考下当我们遇到以下情况该如何去做？ 网站瞬间 QPS 非常高，这是我们为了解决这种瞬间流量是扩机器解决？还是放任不管等待服务恢复？ 在微服务流行的如今，如何保证分布式事务的最终一致性？ 当你遇到这种问题时，很多情况下我们首先能想到的选择里，一定都会有消息队列。 通常来说，使用消息队列能为我们的系统带来下面三点好处： 通过异步处理提高系统性能（减少响应所需时间）。 削峰/限流 降低系统耦合性。 2.1 通过异步处理，提高服务响应时间。 用户调用一个接口的时候，可能该接口调用了别的方法。例如：用户注册的时候，后台可能需要调用：查询数据库，插入数据库，发送邮件，发送用户指南等等... 但是用户可能并不需要后台将所有的任务执行完毕，那么此时在初入数据口后面加入mq队列，用户就能很快得到注册成功的响应而去做一些别的事情。mq的机制又能保证最终的一致性，所以使用起来很安全很稳定。 2.2 削峰/限流 拿笔者目前从事的工作来说，一个订单下单的流程设计到：商品、物流、IM、支付等系统。后端在收到用户交易后，可以先响应用户下单成功这以结果，然后将消息投递到 MQ ，等待系统慢慢消费用户下单这一消息。 2.3 降低系统耦合性 生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合，这显然也提高了系统的扩展性。 3. 使用消息队列我们需要关注的一些问题 系统引入任何框架，它解决了你当时所面临的问题以外，也会带来一些问题。所以我们程序员引入框架时，一定要做好前期调研。 系统可用性降低： 在没有引入 MQ 之前，你可能只需要关注 Java 应用和 数据库 运行稳定就好了。但是当你引入 MQ 之后，你还需要关注 MQ 服务异常消息丢失、 MQ 服务挂起导致整个系统无法工作等情况。 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! 4. 消息队列的两种协议 JMS 、 AMQP 4.1 JMS 4.1.1 JMS 简介 JMS（JAVA Message Service,java 消息服务）是 java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS（JAVA Message Service，Java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 ActiveMQ 就是基于 JMS 规范实现的。 4.1.2 JMS 两种消息模型 ① 点到点（P2P）模型 使用队列（Queue）作为消息通信载体；满足*生产者与消费者模式*，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） ② 发布/订阅（Pub/Sub）模型 发布订阅模型（Pub/Sub） 使用主题（Topic）作为消息通信载体，类似于*广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的*。 4.1.3 JMS 五种不同的消息正文格式 JMS 定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage -- Java 原始值的数据流 MapMessage--一套名称-值对 TextMessage--一个字符串对象 ObjectMessage--一个序列化的 Java 对象 BytesMessage--一个字节的数据流 4.2 AMQP AMQP，即 Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准 高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。 RabbitMQ 就是基于 AMQP 协议实现的。 4.3 JMS vs AMQP 对比方向 JMS AMQP 定义 Java API 协议 跨语言 否 是 跨平台 否 是 支持消息类型 提供两种消息模型：①Peer-2-Peer;②Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分； 支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制） 总结： AMQP 为消息定义了线路层（wire-level protocol）的协议，而 JMS 所定义的是 API 规范。在 Java 体系中，多个 client 均可以通过 JMS 进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而 AMQP 天然具有跨平台、跨语言特性。 JMS 支持 TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于 Exchange 提供的路由算法，AMQP 可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题/订阅 方式两种。 5. 常见的消息队列对比 activeMQ rabbitMQ RocketMQ Kafka 并发量 万级 万级 十万级 万级 响应时长 毫秒 微秒 毫秒 毫秒 开发语言 Java Erlang Java Java Scale 功能完备 完备 完备 完备 不完备 常用场景 小型项目demo * * 大数据领域日志处理、实时计算 社区 低 高 一般 高 总结： RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做 erlang 源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的 MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。 Kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 "},"doc/messagequeue/rocketmq_1.html":{"url":"doc/messagequeue/rocketmq_1.html","title":"rocketMQ 消息发送流程","keywords":"","body":"RocketMQ 消息发送流程 基于 RocketMQ 4.2.0 版本进行的源码分析。 本文讲述 RocketMQ 发送一条普通消息的流程。 一、服务器启动 我们可以参考官方文档来启动服务: 启动 Name 服务器: sh bin/mqnamesrv 启动 Broker 服务器: sh bin/mqbroker -n localhost:9876 二、构建消息体 一条消息体最少需要指定两个值: 所属话题 消息内容 如下就是创建了一条话题为 “Test”，消息体为 “Hello World” 的消息: Message msg = new Message( \"Test\", \"Hello World\".getBytes() ); 三、启动 Producer 准备发送消息 如果我们想要发送消息呢，我们还需要再启动一个 DefaultProducer (生产者) 类来发消息: DefaultMQProducer producer = new DefaultMQProducer(); producer.start(); 现在我们所启动的服务如下所示: 四、Name 服务器的均等性 注意我们上述开启的是单个服务，也即一个 Broker 和一个 Name 服务器，但是实际上使用消息队列的时候，我们可能需要搭建的是一个集群，如下所示: 在 RocketMQ 的设计中，客户端需要首先询问 Name 服务器才能确定一个合适的 Broker 以进行消息的发送: 然而这么多 Name 服务器，客户端是如何选择一个合适的 Name 服务器呢? 首先，我们要意识到很重要的一点，Name 服务器全部都是处于相同状态的，保存的都是相同的信息。在 Broker 启动的时候，其会将自己在本地存储的话题配置文件 (默认位于 $HOME/store/config/topics.json 目录) 中的所有话题加载到内存中去，然后会将这些所有的话题全部同步到所有的 Name 服务器中。与此同时，Broker 也会启动一个定时任务，默认每隔 30 秒来执行一次话题全同步: 五、选择 Name 服务器 由于 Name 服务器每台机器存储的数据都是一致的。因此我们客户端任意选择一台服务器进行沟通即可。 其中客户端一开始选择 Name 服务器的源码如下所示: public class NettyRemotingClient extends NettyRemotingAbstract implements RemotingClient { private final AtomicInteger namesrvIndex = new AtomicInteger(initValueIndex()); private static int initValueIndex() { Random r = new Random(); return Math.abs(r.nextInt() % 999) % 999; } private Channel getAndCreateNameserverChannel() throws InterruptedException { // ... for (int i = 0; i 以后，如果 namesrvAddrChoosed 选择的服务器如果一直处于连接状态，那么客户端就会一直与这台服务器进行沟通。否则的话，如上源代码所示，就会自动轮寻下一台可用服务器。 六、寻找话题路由信息 当客户端发送消息的时候，其首先会尝试寻找话题路由信息。即这条消息应该被发送到哪个地方去。 客户端在内存中维护了一份和话题相关的路由信息表 topicPublishInfoTable，当发送消息的时候，会首先尝试从此表中获取信息。如果此表不存在这条话题的话，那么便会从 Name 服务器获取路由消息。 public class DefaultMQProducerImpl implements MQProducerInner { private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) { TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic); if (null == topicPublishInfo || !topicPublishInfo.ok()) { this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo()); this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic); topicPublishInfo = this.topicPublishInfoTable.get(topic); } // ... } } 当尝试从 Name 服务器获取路由信息的时候，其可能会返回两种情况: (1) 新建话题 这个话题是新创建的，Name 服务器不存在和此话题相关的信息： (2) 已存话题 话题之前创建过，Name 服务器存在此话题信息： 服务器返回的话题路由信息包括以下内容: “broker-1”、”broker-2” 分别为两个 Broker 服务器的名称，相同名称下可以有主从 Broker，因此每个 Broker 又都有 brokerId 。默认情况下，BrokerId 如果为 MixAll.MASTER_ID （值为 0） 的话，那么认为这个 Broker 为 MASTER 主机，其余的位于相同名称下的 Broker 为这台 MASTER 主机的 SLAVE 主机。 public class MQClientInstance { public String findBrokerAddressInPublish(final String brokerName) { HashMap map = this.brokerAddrTable.get(brokerName); if (map != null && !map.isEmpty()) { return map.get(MixAll.MASTER_ID); } return null; } } 每个 Broker 上面可以绑定多个可写消息队列和多个可读消息队列，客户端根据返回的所有 Broker 地址列表和每个 Broker 的可写消息队列列表会在内存中构建一份所有的消息队列列表。之后客户端每次发送消息，都会在消息队列列表上轮循选择队列 (我们假设返回了两个 Broker，每个 Broker 均有 4 个可写消息队列): public class TopicPublishInfo { public MessageQueue selectOneMessageQueue() { int index = this.sendWhichQueue.getAndIncrement(); int pos = Math.abs(index) % this.messageQueueList.size(); if (pos 七、给 Broker 发送消息 在确定了 Master Broker 地址和这个 Broker 的消息队列以后，客户端才开始真正地发送消息给这个 Broker，也是从这里客户端才开始与 Broker 进行交互: 这里我们暂且先忽略消息体格式的具体编/解码过程，因为我们并不想一开始就卷入这些繁枝细节中，现在先从大体上了解一下整个消息的发送流程，后续会写专门的文章来说明。 八、Broker 检查话题信息 刚才说到，如果话题信息在 Name 服务器不存在的话，那么会使用默认话题信息进行消息的发送。然而一旦这条消息到来之后，Broker 端还并没有这个话题。所以 Broker 需要检查话题的存在性: public abstract class AbstractSendMessageProcessor implements NettyRequestProcessor { protected RemotingCommand msgCheck(final ChannelHandlerContext ctx, final SendMessageRequestHeader requestHeader, final RemotingCommand response) { // ... TopicConfig topicConfig = this.brokerController .getTopicConfigManager() .selectTopicConfig(requestHeader.getTopic()); if (null == topicConfig) { // ... topicConfig = this.brokerController .getTopicConfigManager() .createTopicInSendMessageMethod( ... ); } } } 如果话题不存在的话，那么便会创建一个话题信息存储到本地，并将所有话题再进行一次同步给所有的 Name 服务器: public class TopicConfigManager extends ConfigManager { public TopicConfig createTopicInSendMessageMethod(final String topic, /** params **/) { // ... topicConfig = new TopicConfig(topic); this.topicConfigTable.put(topic, topicConfig); this.persist(); // ... this.brokerController.registerBrokerAll(false, true); return topicConfig; } } 话题检查的整体流程如下所示: 九、消息存储 当 Broker 对消息的一些字段做过一番必要的检查之后，便会存储到磁盘中去: 十、整体流程 发送消息的整体流程: "},"doc/messagequeue/rocketmq_2.html":{"url":"doc/messagequeue/rocketmq_2.html","title":"rocketMQ 消息储存流程","keywords":"","body":"本文讲述 RocketMQ 存储一条消息的流程。 一、存储位置 当有一条消息过来之后，Broker 首先需要做的是确定这条消息应该存储在哪个文件里面。在 RocketMQ 中，这个用来存储消息的文件被称之为 MappedFile。这个文件默认创建的大小为 1GB。 一个文件为 1GB 大小，也即 1024 * 1024 * 1024 = 1073741824 字节，这每个文件的命名是按照总的字节偏移量来命名的。例如第一个文件偏移量为 0，那么它的名字为 00000000000000000000；当当前这 1G 文件被存储满了之后，就会创建下一个文件，下一个文件的偏移量则为 1GB，那么它的名字为 00000000001073741824，以此类推。 默认情况下这些消息文件位于 $HOME/store/commitlog 目录下，如下图所示: 二、文件创建 当 Broker 启动的时候，其会将位于存储目录下的所有消息文件加载到一个列表中: 当有新的消息到来的时候，其会默认选择列表中的最后一个文件来进行消息的保存: public class MappedFileQueue { public MappedFile getLastMappedFile() { MappedFile mappedFileLast = null; while (!this.mappedFiles.isEmpty()) { try { mappedFileLast = this.mappedFiles.get(this.mappedFiles.size() - 1); break; } catch (IndexOutOfBoundsException e) { //continue; } catch (Exception e) { log.error(\"getLastMappedFile has exception.\", e); break; } } return mappedFileLast; } } 当然如果这个 Broker 之前从未接受过消息的话，那么这个列表肯定是空的。这样一旦有新的消息需要存储的时候，其就得需要立即创建一个 MappedFile 文件来存储消息。 RocketMQ 提供了一个专门用来实例化 MappedFile 文件的服务类 AllocateMappedFileService。在内存中，也同时维护了一张请求表 requestTable 和一个优先级请求队列 requestQueue 。当需要创建文件的时候，Broker 会创建一个 AllocateRequest 对象，其包含了文件的路径、大小等信息。然后先将其放入 requestTable 表中，再将其放入优先级请求队列 requestQueue 中: public class AllocateMappedFileService extends ServiceThread { public MappedFile putRequestAndReturnMappedFile(String nextFilePath, String nextNextFilePath, int fileSize) { // ... AllocateRequest nextReq = new AllocateRequest(nextFilePath, fileSize); boolean nextPutOK = this.requestTable.putIfAbsent(nextFilePath, nextReq) == null; if (nextPutOK) { // ... boolean offerOK = this.requestQueue.offer(nextReq); } } } 服务类会一直等待优先级队列是否有新的请求到来，如果有，便会从队列中取出请求，然后创建对应的 MappedFile，并将请求表 requestTable 中 AllocateRequest 对象的字段 mappedFile 设置上值。最后将 AllocateRequest 对象上的 CountDownLatch 的计数器减 1 ，以标明此分配申请的 MappedFile 已经创建完毕了: public class AllocateMappedFileService extends ServiceThread { public void run() { // 一直运行 while (!this.isStopped() && this.mmapOperation()) { } } private boolean mmapOperation() { req = this.requestQueue.take(); if (req.getMappedFile() == null) { MappedFile mappedFile; // ... mappedFile = new MappedFile(req.getFilePath(), req.getFileSize()); // 设置上值 req.setMappedFile(mappedFile); } // ... // 计数器减 1 req.getCountDownLatch().countDown(); // ... return true; } } 其上述整体流程如下所示: 等待 MappedFile 创建完毕之后，其便会从请求表 requestTable 中取出并删除表中记录: public class AllocateMappedFileService extends ServiceThread { public MappedFile putRequestAndReturnMappedFile(String nextFilePath, String nextNextFilePath, int fileSize) { // ... AllocateRequest result = this.requestTable.get(nextFilePath); if (result != null) { // 等待 MappedFile 的创建完成 boolean waitOK = result.getCountDownLatch().await(waitTimeOut, TimeUnit.MILLISECONDS); if (!waitOK) { return null; } else { // 从请求表中删除 this.requestTable.remove(nextFilePath); return result.getMappedFile(); } } } } 然后再将其放到列表中去: public class MappedFileQueue { public MappedFile getLastMappedFile(final long startOffset, boolean needCreate) { MappedFile mappedFile = null; if (this.allocateMappedFileService != null) { // 创建 MappedFile mappedFile = this.allocateMappedFileService .putRequestAndReturnMappedFile(nextFilePath, nextNextFilePath, this.mappedFileSize); } if (mappedFile != null) { // ... // 添加至列表中 this.mappedFiles.add(mappedFile); } return mappedFile; } } 至此，MappedFile 已经创建完毕，也即可以进行下一步的操作了。 三、文件初始化 在 MappedFile 的构造函数中，其使用了 FileChannel 类提供的 map 函数来将磁盘上的这个文件映射到进程地址空间中。然后当通过 MappedByteBuffer 来读入或者写入文件的时候，磁盘上也会有相应的改动。采用这种方式，通常比传统的基于文件 IO 流的方式读取效率高。 public class MappedFile extends ReferenceResource { public MappedFile(final String fileName, final int fileSize) throws IOException { init(fileName, fileSize); } private void init(final String fileName, final int fileSize) throws IOException { // ... this.fileChannel = new RandomAccessFile(this.file, \"rw\").getChannel(); this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize); // ... } } 四、消息文件加载 前面提到过，Broker 在启动的时候，会加载磁盘上的文件到一个 mappedFiles 列表中。但是加载完毕后，其还会对这份列表中的消息文件进行验证 (恢复)，确保没有错误。 验证的基本想法是通过一一读取列表中的每一个文件，然后再一一读取每个文件中的每个消息，在读取的过程中，其会更新整体的消息写入的偏移量，如下图中的红色箭头 (我们假设最终读取的消息的总偏移量为 905): 当确定消息整体的偏移量之后，Broker 便会确定每一个单独的 MappedFile 文件的各自的偏移量，每一个文件的偏移量是通过取余算法确定的: public class MappedFileQueue { public void truncateDirtyFiles(long offset) { for (MappedFile file : this.mappedFiles) { long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize; if (fileTailOffset > offset) { if (offset >= file.getFileFromOffset()) { // 确定每个文件的各自偏移量 file.setWrotePosition((int) (offset % this.mappedFileSize)); file.setCommittedPosition((int) (offset % this.mappedFileSize)); file.setFlushedPosition((int) (offset % this.mappedFileSize)); } else { // ... } } } // ... } } 在确定每个消息文件各自的写入位置的同时，其还会删除起始偏移量大于当前总偏移量的消息文件，这些文件可以视作脏文件，或者也可以说这些文件里面一条消息也没有。这也是上述文件 1073741824 被打上红叉的原因: public void truncateDirtyFiles(long offset) { List willRemoveFiles = new ArrayList(); for (MappedFile file : this.mappedFiles) { long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize; if (fileTailOffset > offset) { if (offset >= file.getFileFromOffset()) { // ... } else { // 总偏移量 五、写入消息 一旦我们获取到 MappedFile 文件之后，我们便可以往这个文件里面写入消息了。写入消息可能会遇见如下两种情况，一种是这条消息可以完全追加到这个文件中，另外一种是这条消息完全不能或者只有一小部分只能存放到这个文件中，其余的需要放到新的文件中。我们对于这两种情况分别讨论: (1) 文件可以完全存储消息 MappedFile 类维护了一个用以标识当前写位置的指针 wrotePosition，以及一个用来映射文件到进程地址空间的 mappedByteBuffer: public class MappedFile extends ReferenceResource { protected final AtomicInteger wrotePosition = new AtomicInteger(0); private MappedByteBuffer mappedByteBuffer; } 由这两个数据结构我们可以看出来，单个文件的消息写入过程其实是非常简单的。首先获取到这个文件的写入位置，然后将消息内容追加到 byteBuffer 中，然后再更新写入位置。 public class MappedFile extends ReferenceResource { public AppendMessageResult appendMessagesInner(final MessageExt messageExt, final AppendMessageCallback cb) { // ... int currentPos = this.wrotePosition.get(); if (currentPos 示例流程如下所示: (2) 文件不可以完全存储消息 在写入消息之前，如果判断出文件已经满了的情况下，其会直接尝试创建一个新的 MappedFile: public class CommitLog { public PutMessageResult putMessage(final MessageExtBrokerInner msg) { // 文件为空 || 文件已经满了 if (null == mappedFile || mappedFile.isFull()) { mappedFile = this.mappedFileQueue.getLastMappedFile(0); } // ... result = mappedFile.appendMessage(msg, this.appendMessageCallback); } } 如果文件未满，那么在写入之前会先计算出消息体长度 msgLen，然后判断这个文件剩下的空间是否有能力容纳这条消息。在这个地方我们还需要介绍下每条消息的存储方式。 每条消息的存储是按照一个 4 字节的长度来做界限的，这个长度本身就是整个消息体的长度，当读完这整条消息体的长度之后，下一次再取出来的一个 4 字节的数字，便又是下一条消息的长度: 围绕着一条消息，还会存储许多其它内容，我们在这里只需要了解前两位是 4 字节的长度和 4 字节的 MAGICCODE 即可: MAGICCODE 的可选值有: CommitLog.MESSAGE_MAGIC_CODE CommitLog.BLANK_MAGIC_CODE 当这个文件有能力容纳这条消息体的情况下，其便会存储 MESSAGE_MAGIC_CODE 值；当这个文件没有能力容纳这条消息体的情况下，其便会存储 BLANK_MAGIC_CODE 值。所以这个 MAGICCODE 是用来界定这是空消息还是一条正常的消息。 当判定这个文件不足以容纳整个消息的时候，其将消息体长度设置为这个文件剩余的最大空间长度，将 MAGICCODE 设定为这是一个空消息文件 (需要去下一个文件去读)。由此我们可以看出消息体长度 和 MAGICCODE 是判别一条消息格式的最基本要求，这也是 END_FILE_MIN_BLANK_LENGTH 的值为 8 的原因: // CommitLog.java class DefaultAppendMessageCallback implements AppendMessageCallback { // File at the end of the minimum fixed length empty private static final int END_FILE_MIN_BLANK_LENGTH = 4 + 4; public AppendMessageResult doAppend(final long fileFromOffset, final ByteBuffer byteBuffer, final int maxBlank, final MessageExtBrokerInner msgInner) { // ... if ((msgLen + END_FILE_MIN_BLANK_LENGTH) > maxBlank) { // ... // 1 TOTALSIZE this.msgStoreItemMemory.putInt(maxBlank); // 2 MAGICCODE this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE); // 3 The remaining space may be any value byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank); return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, /** other params **/ ); } } } 由上述方法我们看出在这种情况下返回的结果是 END_OF_FILE。当检测到这种返回结果的时候，CommitLog 接着又会申请创建新的 MappedFile 并尝试写入消息。追加方法同 (1) 相同，不再赘述: 注: 在消息文件加载的过程中，其也是通过判断 MAGICCODE 的类型，来判断是否继续读取下一个 MappedFile 来计算整体消息偏移量的。 六、消息刷盘策略 当消息体追加到 MappedFile 以后，这条消息实际上还只是存储在内存中，因此还需要将内存中的内容刷到磁盘上才算真正的存储下来，才能确保消息不丢失。一般而言，刷盘有两种策略: 异步刷盘和同步刷盘。 (1) 异步刷盘 当配置为异步刷盘策略的时候，Broker 会运行一个服务 FlushRealTimeService 用来刷新缓冲区的消息内容到磁盘，这个服务使用一个独立的线程来做刷盘这件事情，默认情况下每隔 500ms 来检查一次是否需要刷盘: class FlushRealTimeService extends FlushCommitLogService { public void run() { // 不停运行 while (!this.isStopped()) { // interval 默认值是 500ms if (flushCommitLogTimed) { Thread.sleep(interval); } else { this.waitForRunning(interval); } // 刷盘 CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages); } } } 在追加消息完毕之后，通过唤醒这个服务立即检查以下是否需要刷盘: public class CommitLog { public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) { // Synchronization flush if (FlushDiskType.SYNC_FLUSH == this.defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) { // ... } // Asynchronous flush else { if (!this.defaultMessageStore.getMessageStoreConfig().isTransientStorePoolEnable()) { // 消息追加成功后，立即唤醒服务 flushCommitLogService.wakeup(); } else { // ... } } } } (2) 同步刷盘 当配置为同步刷盘策略的时候，Broker 运行一个叫做 GroupCommitService 服务。在这个服务内部维护了一个写请求队列和一个读请求队列，其中这两个队列每隔 10ms 就交换一下“身份”，这么做的目的其实也是为了读写分离: 在这个服务内部，每隔 10ms 就会检查读请求队列是否不为空，如果不为空，则会将读队列中的所有请求执行刷盘，并清空读请求队列: class GroupCommitService extends FlushCommitLogService { private void doCommit() { // 检查所有读队列中的请求 for (GroupCommitRequest req : this.requestsRead) { // 每个请求执行刷盘 CommitLog.this.mappedFileQueue.flush(0); req.wakeupCustomer(flushOK); } this.requestsRead.clear(); } } 在追加消息完毕之后，通过创建一个请求刷盘的对象，然后通过 putRequest() 方法放入写请求队列中，这个时候会立即唤醒这个服务，写队列和读队列的角色会进行交换，交换角色之后，读请求队列就不为空，继而可以执行所有刷盘请求了。而在这期间，Broker 会一直阻塞等待最多 5 秒钟，在这期间如果完不成刷盘请求的话，那么视作刷盘超时: public class CommitLog { public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) { // Synchronization flush if (FlushDiskType.SYNC_FLUSH == this.defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) { // ... if (messageExt.isWaitStoreMsgOK()) { GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes()); service.putRequest(request); // 等待刷盘成功 boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout()); if (!flushOK) { // 刷盘超时 putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT); } } else { // ... } } // Asynchronous flush else { // ... } } } 通过方法 putRequest 放入请求后的服务执行流程: 七、消息刷盘理念 我们在这里已经知道消息刷盘有同步刷盘和异步刷盘策略，对应的是 GroupCommitService 和 FlushRealTimeService 这两种不同的服务。 这两种服务都有定时请求刷盘的机制，但是机制背后最终调用的刷盘方式全部都集中在 flush 这个方法上: public class MappedFileQueue { public boolean flush(final int flushLeastPages) { // ... } } 再继续向下分析这个方法之前，我们先对照着这张图说明一下使用 MappedByteBuffer 来简要阐述读和写文件的简单过程： 操作系统为了能够使多个进程同时使用内存，又保证各个进程访问内存互相独立，于是为每个进程引入了地址空间的概念，地址空间上的地址叫做虚拟地址，而程序想要运行必须放到物理地址上运行才可以。地址空间为进程营造出了一种假象：”整台计算机只有我一个程序在运行，这台计算机内存很大”。一个地址空间内包含着这个进程所需要的全部状态信息。通常一个进程的地址空间会按照逻辑分成好多段，比如代码段、堆段、栈段等。为了进一步有效利用内存，每一段又细分成了不同的页 (page)。与此相对应，计算机的物理内存被切成了页帧 (page frame)，文件被分成了块 (block)。既然程序实际运行的时候还是得依赖物理内存的地址，那么就需要将虚拟地址转换为物理地址，这个映射关系是由页表 (page table)来完成的。 另外在操作系统中，还有一层磁盘缓存 (disk cache)\\的概念，它主要是用来减少对磁盘的 I/O 操作。磁盘缓存是以页为单位的，内容就是磁盘上的物理块，所以又称之为*页缓存 (page cache)*。当进程发起一个读操作 （比如，进程发起一个 read() 系统调用），它首先会检查需要的数据是否在页缓存中。如果在，则放弃访问磁盘，而直接从页缓存中读取。如果数据没在缓存中，那么内核必须调度块 I/O 操作从磁盘去读取数据，然后将读来的数据放入页缓存中。系统并不一定要将整个文件都缓存，它可以只存储一个文件的一页或者几页。 如图所示，当调用 FileChannel.map() 方法的时候，会将这个文件映射进用户空间的地址空间中，注意，建立映射不会拷贝任何数据。我们前面提到过 Broker 启动的时候会有一个消息文件加载的过程，当第一次开始读取数据的时候: // 首次读取数据 int totalSize = byteBuffer.getInt(); 这个时候，操作系统通过查询页表，会发现文件的这部分数据还不在内存中。于是就会触发一个缺页异常 (page faults)，这个时候操作系统会开始从磁盘读取这一页数据，然后先放入到页缓存中，然后再放入内存中。在第一次读取文件的时候，操作系统会读入所请求的页面，并读入紧随其后的少数几个页面（不少于一个页面，通常是三个页面），这时的预读称为同步预读 (如下图所示，红色部分是需要读取的页面，蓝色的那三个框是操作系统预先读取的): 当然随着时间推移，预读命中的话，那么相应的预读页面数量也会增加，但是能够确认的是，一个文件至少有 4 个页面处在页缓存中。当文件一直处于顺序读取的情况下，那么基本上可以保证每次预读命中: 下面我们来说文件写，正常情况下，当尝试调用 writeInt() 写数据到文件里面的话，其写到页缓存层，这个方法就会返回了。这个时候数据还没有真正的保存到文件中去，Linux 仅仅将页缓存中的这一页数据标记为“脏”，并且被加入到脏页链表中。然后由一群进程（flusher 回写进程）周期性将脏页链表中的页写会到磁盘，从而让磁盘中的数据和内存中保持一致，最后清理“脏”标识。在以下三种情况下，脏页会被写回磁盘: 空闲内存低于一个特定阈值 脏页在内存中驻留超过一个特定的阈值时 当用户进程调用 sync() 和 fsync() 系统调用时 可见，在正常情况下，即使不采用刷盘策略，数据最终也是会被同步到磁盘中去的: 但是，即便有 flusher 线程来定时同步数据，如果此时机器断电的话，消息依然有可能丢失。RocketMQ 为了保证消息尽可能的不丢失，为了最大的高可靠性，做了同步和异步刷盘策略，来手动进行同步: 八、消息刷盘过程 在介绍完上述消息刷盘背后的一些机制和理念后，我们再来分析刷盘整个过程。首先，无论同步刷盘还是异步刷盘，其线程都在一直周期性的尝试执行刷盘，在真正执行刷盘函数的调用之前，Broker 会检查文件的写位置是否大于 flush 位置，避免执行无意义的刷盘： 其次，对于异步刷盘来讲，Broker 执行了更为严格的刷盘限制策略，当在某个时间点尝试执行刷盘之后，在接下来 10 秒内，如果想要继续刷盘，那么脏页面数量必须不小于 4 页，如下图所示: 下面是执行刷盘前最后检查的刷盘条件： public class MappedFile extends ReferenceResource { private boolean isAbleToFlush(final int flushLeastPages) { int flush = this.flushedPosition.get(); int write = getReadPosition(); if (this.isFull()) { return true; } if (flushLeastPages > 0) { // 计算当前脏页面算法 return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) >= flushLeastPages; } // wrotePosition > flushedPosition return write > flush; } } 当刷盘完毕之后，首先会更新这个文件的 flush 位置，然后再更新 MappedFileQueue 的整体的 flush 位置: 当刷盘完毕之后，便会将结果通知给客户端，告知发送消息成功。至此，整个存储过程完毕。 "},"doc/messagequeue/rocketmq_3.html":{"url":"doc/messagequeue/rocketmq_3.html","title":"rocketMQ 消息消费流程","keywords":"","body":"本篇讲述 RocketMQ 消息接受流程 一、消费者注册 生产者负责往服务器 Broker 发送消息，消费者则从 Broker 获取消息。消费者获取消息采用的是订阅者模式，即消费者客户端可以任意订阅一个或者多个话题来消费消息: public class Consumer { public static void main(String[] args) throws InterruptedException, MQClientException { /* * 订阅一个或者多个话题 */ consumer.subscribe(\"TopicTest\", \"*\"); } } 当消费者客户端启动以后，其会每隔 30 秒从命名服务器查询一次用户订阅的所有话题路由信息: public class MQClientInstance { private void startScheduledTask() { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { // 从命名服务器拉取话题信息 MQClientInstance.this.updateTopicRouteInfoFromNameServer(); } }, 10, this.clientConfig.getPollNameServerInterval(), TimeUnit.MILLISECONDS); } } 我们由 RocketMQ 消息发送流程 这篇文章知道 RocketMQ 在发送消息的时候，每条消息会以轮循的方式均衡地分发的不同 Broker 的不同队列中去。由此，消费者客户端从服务器命名服务器获取下来的便是话题的所有消息队列: 在获取话题路由信息的时候，客户端还会将话题路由信息中的所有 Broker 地址保存到本地: public class MQClientInstance { public boolean updateTopicRouteInfoFromNameServer(final String topic, boolean isDefault, DefaultMQProducer defaultMQProducer) { // ... if (changed) { TopicRouteData cloneTopicRouteData = topicRouteData.cloneTopicRouteData(); // 更新 Broker 地址列表 for (BrokerData bd : topicRouteData.getBrokerDatas()) { this.brokerAddrTable.put(bd.getBrokerName(), bd.getBrokerAddrs()); } return true; } // ... } } 当消费者客户端获取到了 Broker 地址列表之后，其便会每隔 30 秒给服务器发送一条心跳数据包，告知所有 Broker 服务器这台消费者客户端的存在。在每次发送心跳包的同时，其数据包内还会捎带这个客户端消息订阅的一些组信息，比如用户订阅了哪几个话题等，与此相对应，每台 Broker 服务器会在内存中维护一份当前所有的消费者客户端列表信息: public class ConsumerManager { private final ConcurrentMap consumerTable = new ConcurrentHashMap(1024); } 消费者客户端与 Broker 服务器进行沟通的整体流程如下图所示： 二、消息队列负载均衡 我们知道无论发送消息还是接受消息都需要指定消息的话题，然而实际上消息在 Broker 服务器上并不是以话题为单位进行存储的，而是采用了比话题更细粒度的队列来进行存储的。当你发送了 10 条相同话题的消息，这 10 条话题可能存储在了不同 Broker 服务器的不同队列中。由此，我们说 RocketMQ 管理消息的单位不是话题，而是队列。 当我们讨论消息队列负载均衡的时候，就是在讨论服务器端的所有队列如何给所有消费者消费的问题。在 RocketMQ 中，客户端有两种消费模式，一种是广播模式，另外一种是集群模式。 我们现在假设总共有两台 Broker 服务器，假设用户使用 Producer 已经发送了 8 条消息，这 8 条消息现在均衡的分布在两台 Broker 服务器的 8 个队列中，每个队列中有一个消息。现在有 3 台都订阅了 Test 话题的消费者实例，我们来看在不同消费模式下，不同的消费者会收到哪几条消息。 (1) 广播模式 广播模式是指所有消息队列中的消息都会广播给所有的消费者客户端，如下图所示，每一个消费者都能收到这 8 条消息: (2) 集群模式 集群模式是指所有的消息队列会按照某种分配策略来分给不同的消费者客户端，比如消费者 A 消费前 3 个队列中的消息，消费者 B 消费中间 3 个队列中的消息等等。我们现在着重看 RocketMQ 为我们提供的三个比较重要的消息队列分配策略: 1. 平均分配策略 平均分配策略下，三个消费者的消费情况如下所示： Consumer-1 消费前 3 个消息队列中的消息 Consumer-2 消费中间 3 个消息队列中的消息 Consumer-3 消费最后 2 个消息队列中的消息 2. 平均分配轮循策略 平均分配轮循策略下，三个消费者的消费情况如下所示： Consumer-1 消费 1、4、7消息队列中的消息 Consumer-2 消费 2、5、8消息队列中的消息 Consumer-3 消费 3、6消息队列中的消息 3. 一致性哈希策略 一致性哈希算法是根据这三台消费者各自的某个有代表性的属性(我们假设就是客户端ID)来计算出三个 Hash 值，此处为了减少由于 Hash 函数选取的不理想的情况， RocketMQ 算法对于每个消费者通过在客户端ID后面添加 1、2、3 索引来使每一个消费者多生成几个哈希值。那么现在我们需要哈希的就是九个字符串: Consumer-1-1 Consumer-1-2 Consumer-1-3 Consumer-2-1 Consumer-2-2 Consumer-2-3 Consumer-3-1 Consumer-3-2 Consumer-3-3 计算完这 9 个哈希值以后，我们按照从小到大的顺序来排列成一个环 (如图所示)。这个时候我们需要一一对这 8 个消息队列也要计算一下 Hash 值，当 Hash 值落在两个圈之间的时候，我们就选取沿着环的方向的那个节点作为这个消息队列的消费者。如下图所示 (注意: 图只是示例，并非真正的消费情况): 在一致性哈希策略下，三个消费者的消费情况如下所示： Consumer-1 消费 1、2、3、4消息队列中的消息 Consumer-2 消费 5、8消息队列中的消息 Consumer-3 消费 6、7消息队列中的消息 消息队列的负载均衡是由一个不停运行的均衡服务来定时执行的: public class RebalanceService extends ServiceThread { // 默认 20 秒一次 private static long waitInterval = Long.parseLong(System.getProperty(\"rocketmq.client.rebalance.waitInterval\", \"20000\")); @Override public void run() { while (!this.isStopped()) { this.waitForRunning(waitInterval); // 重新执行消息队列的负载均衡 this.mqClientFactory.doRebalance(); } } } 接着往下看，会知道在广播模式下，当前这台消费者消费和话题相关的所有消息队列，而集群模式会先按照某种分配策略来进行消息队列的分配，得到的结果就是当前这台消费者需要消费的消息队列: public abstract class RebalanceImpl { private void rebalanceByTopic(final String topic, final boolean isOrder) { switch (messageModel) { // 广播模式 case BROADCASTING: { // 消费这个话题的所有消息队列 Set mqSet = this.topicSubscribeInfoTable.get(topic); if (mqSet != null) { // ... } break; } // 集群模式 case CLUSTERING: { // ... // 按照某种负载均衡策略进行消息队列和消费客户端之间的分配 // allocateResult 就是当前这台消费者被分配到的消息队列 allocateResult = strategy.allocate( this.consumerGroup, this.mQClientFactory.getClientId(), mqAll, cidAll); // ... } break; } } } 三、Broker 消费队列文件 现在我们再来看 Broker 服务器端。首先我们应该知道，消息往 Broker 存储就是在向 CommitLog 消息文件中写入数据的一个过程。在 Broker 启动过程中，其会启动一个叫做 ReputMessageService 的服务，这个服务每隔 1 秒会检查一下这个 CommitLog 是否有新的数据写入。ReputMessageService 自身维护了一个偏移量 reputFromOffset，用以对比和 CommitLog 文件中的消息总偏移量的差距。当这两个偏移量不同的时候，就代表有新的消息到来了: class ReputMessageService extends ServiceThread { private volatile long reputFromOffset = 0; private boolean isCommitLogAvailable() { // 看当前有没有新的消息到来 return this.reputFromOffset 在有新的消息到来之后，doReput() 函数会取出新到来的所有消息，每一条消息都会封装为一个 DispatchRequest 请求，进而将这条请求分发给不同的请求消费者，我们在这篇文章中只会关注利用消息创建消费队列的服务 CommitLogDispatcherBuildConsumeQueue: class ReputMessageService extends ServiceThread { // ... 部分代码有删减 private void doReput() { SelectMappedBufferResult result = DefaultMessageStore.this.commitLog.getData(reputFromOffset); if (result != null) { this.reputFromOffset = result.getStartOffset(); for (int readSize = 0; readSize CommitLogDispatcherBuildConsumeQueue 服务会根据这条请求按照不同的队列 ID 创建不同的消费队列文件，并在内存中维护一份消费队列列表。然后将 DispatchRequest 请求中这条消息的消息偏移量、消息大小以及消息在发送时候附带的标签的 Hash 值写入到相应的消费队列文件中去。 消费队列文件的创建与消息存储 CommitLog 文件的创建过程是一致的，只是路径不同，这里不再赘述。 寻找消费队列的代码如下: public class DefaultMessageStore implements MessageStore { private final ConcurrentMap> consumeQueueTable; public void putMessagePositionInfo(DispatchRequest dispatchRequest) { ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId()); cq.putMessagePositionInfoWrapper(dispatchRequest); } } 向消费队列文件中存储数据的代码如下: public class ConsumeQueue { private boolean putMessagePositionInfo(final long offset, final int size, final long tagsCode, final long cqOffset) { // 存储偏移量、大小、标签码 this.byteBufferIndex.flip(); this.byteBufferIndex.limit(CQ_STORE_UNIT_SIZE); this.byteBufferIndex.putLong(offset); this.byteBufferIndex.putInt(size); this.byteBufferIndex.putLong(tagsCode); // 获取消费队列文件 final long expectLogicOffset = cqOffset * CQ_STORE_UNIT_SIZE; MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile(expectLogicOffset); if (mappedFile != null) { // ... return mappedFile.appendMessage(this.byteBufferIndex.array()); } return false; } } 以上阐述了消费队列创建并存储消息的一个过程，但是消费队列文件中的消息是需要持久化到磁盘中去的。持久化的过程是通过后台服务 FlushConsumeQueueService 来定时持久化的: class FlushConsumeQueueService extends ServiceThread { private void doFlush(int retryTimes) { // ... ConcurrentMap> tables = DefaultMessageStore.this.consumeQueueTable; for (ConcurrentMap maps : tables.values()) { for (ConsumeQueue cq : maps.values()) { boolean result = false; for (int i = 0; i 上述过程体现在磁盘文件的变化如下图所示，commitLog 文件夹下面存放的是完整的消息，来一条消息，向文件中追加一条消息。同时，根据这一条消息属于 TopicTest 话题下的哪一个队列，又会往相应的 consumequeue 文件下的相应消费队列文件中追加消息的偏移量、消息大小和标签码: 总流程图如下所示: 四、消息队列偏移量 Broker 服务器存储了各个消费队列，客户端需要消费每个消费队列中的消息。消费模式的不同，每个客户端所消费的消息队列也不同。那么客户端如何记录自己所消费的队列消费到哪里了呢？答案就是消费队列偏移量。 针对同一话题，在集群模式下，由于每个客户端所消费的消息队列不同，所以每个消息队列已经消费到哪里的消费偏移量是记录在 Broker 服务器端的。而在广播模式下，由于每个客户端分配消费这个话题的所有消息队列，所以每个消息队列已经消费到哪里的消费偏移量是记录在客户端本地的。 下面分别讲述两种模式下偏移量是如何获取和更新的: (1) 集群模式 在集群模式下，消费者客户端在内存中维护了一个 offsetTable 表: public class RemoteBrokerOffsetStore implements OffsetStore { private ConcurrentMap offsetTable = new ConcurrentHashMap(); } 同样在 Broker 服务器端也维护了一个偏移量表: public class ConsumerOffsetManager extends ConfigManager { private ConcurrentMap> offsetTable = new ConcurrentHashMap>(512); } 在消费者客户端，RebalanceService 服务会定时地 (默认 20 秒) 从 Broker 服务器获取当前客户端所需要消费的消息队列，并与当前消费者客户端的消费队列进行对比，看是否有变化。对于每个消费队列，会从 Broker 服务器查询这个队列当前的消费偏移量。然后根据这几个消费队列，创建对应的拉取请求 PullRequest 准备从 Broker 服务器拉取消息，如下图所示: 当从 Broker 服务器拉取下来消息以后，只有当用户成功消费的时候，才会更新本地的偏移量表。本地的偏移量表再通过定时服务每隔 5 秒同步到 Broker 服务器端: public class MQClientInstance { private void startScheduledTask() { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { MQClientInstance.this.persistAllConsumerOffset(); } }, 1000 * 10, this.clientConfig.getPersistConsumerOffsetInterval(), TimeUnit.MILLISECONDS); } } 而维护在 Broker 服务器端的偏移量表也会每隔 5 秒钟序列化到磁盘中: public class BrokerController { public boolean initialize() throws CloneNotSupportedException { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { BrokerController.this.consumerOffsetManager.persist(); } }, 1000 * 10, this.brokerConfig.getFlushConsumerOffsetInterval(), TimeUnit.MILLISECONDS); } } 保存的格式如下所示： 上述整体流程如下所示，红框框住的是这个话题下面的队列的 ID，箭头指向的分别是每个队列的消费偏移量： (2) 广播模式 对于广播模式而言，每个消费队列的偏移量肯定不能存储在 Broker 服务器端，因为多个消费者对于同一个队列的消费可能不一致，偏移量会互相覆盖掉。因此，在广播模式下，每个客户端的消费偏移量是存储在本地的，然后每隔 5 秒将内存中的 offsetTable 持久化到磁盘中。当首次从服务器获取可消费队列的时候，偏移量不像集群模式下是从 Broker 服务器读取的，而是直接从本地文件中读取的: public class LocalFileOffsetStore implements OffsetStore { @Override public long readOffset(final MessageQueue mq, final ReadOffsetType type) { if (mq != null) { switch (type) { case READ_FROM_STORE: { // 本地读取 offsetSerializeWrapper = this.readLocalOffset(); // ... } } } // ... } } 当消息消费成功后，偏移量的更新也是持久化到本地，而非更新到 Broker 服务器中。这里提一下，在广播模式下，消息队列的偏移量默认放在用户目录下的 .rocketmq_offsets 目录下: public class LocalFileOffsetStore implements OffsetStore { @Override public void persistAll(Set mqs) { // ... String jsonString = offsetSerializeWrapper.toJson(true); MixAll.string2File(jsonString, this.storePath); // ... } } 存储格式如下： 简要流程图如下： 五、拉取消息 在客户端运行着一个专门用来拉取消息的后台服务 PullMessageService，其接受每个队列创建 PullRequest 拉取消息请求，然后拉取消息: public class PullMessageService extends ServiceThread { @Override public void run() { while (!this.isStopped()) { PullRequest pullRequest = this.pullRequestQueue.take(); if (pullRequest != null) { this.pullMessage(pullRequest); } } } } 每一个 PullRequest 都关联着一个 MessageQueue 和一个 ProcessQueue，在 ProcessQueue 的内部还维护了一个用来等待用户消费的消息树，如下代码所示: public class PullRequest { private MessageQueue messageQueue; private ProcessQueue processQueue; }public class ProcessQueue { private final TreeMap msgTreeMap = new TreeMap(); } 当真正尝试拉取消息之前，其会检查当前请求的内部缓存的消息数量、消息大小、消息阈值跨度是否超过了某个阈值，如果超过某个阈值，则推迟 50 毫秒重新执行这个请求: public class DefaultMQPushConsumerImpl implements MQConsumerInner { public void pullMessage(final PullRequest pullRequest) { // ... final ProcessQueue processQueue = pullRequest.getProcessQueue(); long cachedMessageCount = processQueue.getMsgCount().get(); long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024); // 缓存消息数量阈值，默认为 1000 if (cachedMessageCount > this.defaultMQPushConsumer.getPullThresholdForQueue()) { this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); return; } // 缓存消息大小阈值，默认为 100 MB if (cachedMessageSizeInMiB > this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) { this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); return; } if (!this.consumeOrderly) { // 最小偏移量和最大偏移量的阈值跨度，默认为 2000 偏移量，消费速度不能太慢 if (processQueue.getMaxSpan() > this.defaultMQPushConsumer.getConsumeConcurrentlyMaxSpan()) { this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); return; } } // ... } } 当执行完一些必要的检查之后，客户端会将用户指定的过滤信息以及一些其它必要消费字段封装到请求信息体中，然后才开始从 Broker 服务器拉取这个请求从当前偏移量开始的消息，默认一次性最多拉取 32 条，服务器返回的响应会告诉客户端这个队列下次开始拉取时的偏移量。客户端每次都会注册一个 PullCallback 回调，用以接受服务器返回的响应信息，根据响应信息的不同状态信息，然后修正这个请求的偏移量，并进行下次请求: public void pullMessage(final PullRequest pullRequest) { PullCallback pullCallback = new PullCallback() { @Override public void onSuccess(PullResult pullResult) { if (pullResult != null) { // ... switch (pullResult.getPullStatus()) { case FOUND: // ... break; case NO_NEW_MSG: // ... break; case NO_MATCHED_MSG: // ... break; case OFFSET_ILLEGAL: // ... break; default: break; } } } @Override public void onException(Throwable e) { // ... } }; } 上述是客户端拉取消息时的一些机制，现在再说一下 Broker 服务器端与此相对应的逻辑。 服务器在收到客户端的请求之后，会根据话题和队列 ID 定位到对应的消费队列。然后根据这条请求传入的 offset 消费队列偏移量，定位到对应的消费队列文件。偏移量指定的是消费队列文件的消费下限，而最大上限是由如下算法来进行约束的: final int maxFilterMessageCount = Math.max(16000, maxMsgNums * ConsumeQueue.CQ_STORE_UNIT_SIZE); 有了上限和下限，客户端便会开始从消费队列文件中取出每个消息的偏移量和消息大小，然后再根据这两个值去 CommitLog 文件中寻找相应的完整的消息，并添加到最后的消息队列中，精简过的代码如下所示： public class DefaultMessageStore implements MessageStore { public GetMessageResult getMessage(final String group, final String topic, final int queueId, final long offset, final int maxMsgNums, final MessageFilter messageFilter) { // ... ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId); if (consumeQueue != null) { // 首先根据消费队列的偏移量定位消费队列 SelectMappedBufferResult bufferConsumeQueue = consumeQueue.getIndexBuffer(offset); if (bufferConsumeQueue != null) { try { status = GetMessageStatus.NO_MATCHED_MESSAGE; // 最大消息长度 final int maxFilterMessageCount = Math.max(16000, maxMsgNums * ConsumeQueue.CQ_STORE_UNIT_SIZE); // 取消息 for (; i 客户端和 Broker 服务器端完整拉取消息的流程图如下所示： 六、消费消息 依赖于用户指定的消息回调函数的不同，消息的消费分为两种: 并发消费和有序消费。 并发消费没有考虑消息发送的顺序，客户端从服务器获取到消息就会直接回调给用户。而有序消费会考虑每个队列消息发送的顺序，注意此处并不是每个话题消息发送的顺序，一定要记住 RocketMQ 控制消息的最细粒度是消息队列。当我们讲有序消费的时候，就是在说对于某个话题的某个队列，发往这个队列的消息，客户端接受消息的顺序与发送的顺序完全一致。 下面我们分别看这两种消费模式是如何实现的。 (1) 并发消费 当用户注册消息回调类的时候，如果注册的是 MessageListenerConcurrently 回调类，那么就认为用户不关心消息的顺序问题。我们在上文提到过每个 PullRequest 都关联了一个处理队列 ProcessQueue，而每个处理队列又都关联了一颗消息树 msgTreeMap。当客户端拉取到新的消息以后，其先将消息放入到这个请求所关联的处理队列的消息树中，然后提交一个消息消费请求，用以回调用户端的代码消费消息: public class DefaultMQPushConsumerImpl implements MQConsumerInner { public void pullMessage(final PullRequest pullRequest) { PullCallback pullCallback = new PullCallback() { @Override public void onSuccess(PullResult pullResult) { if (pullResult != null) { switch (pullResult.getPullStatus()) { case FOUND: // 消息放入处理队列的消息树中 boolean dispathToConsume = processQueue .putMessage(pullResult.getMsgFoundList()); // 提交一个消息消费请求 DefaultMQPushConsumerImpl.this .consumeMessageService .submitConsumeRequest( pullResult.getMsgFoundList(), processQueue, pullRequest.getMessageQueue(), dispathToConsume); break; } } } }; } } 当提交一个消息消费请求后，对于并发消费，其实现如下: public class ConsumeMessageConcurrentlyService implements ConsumeMessageService { class ConsumeRequest implements Runnable { @Override public void run() { // ... status = listener.consumeMessage(Collections.unmodifiableList(msgs), context); // ... } } } 我们可以看到 msgs 是直接从服务器端拿到的最新消息，直接喂给了客户端进行消费，并未做任何有序处理。当消费成功后，会从消息树中将这些消息再给删除掉: public class ConsumeMessageConcurrentlyService implements ConsumeMessageService { public void processConsumeResult(final ConsumeConcurrentlyStatus status, /** 其它参数 **/) { // 从消息树中删除消息 long offset = consumeRequest.getProcessQueue().removeMessage(consumeRequest.getMsgs()); if (offset >= 0 && !consumeRequest.getProcessQueue().isDropped()) { this.defaultMQPushConsumerImpl.getOffsetStore() .updateOffset(consumeRequest.getMessageQueue(), offset, true); } } } (2) 有序消费 RocketMQ 的有序消费主要依靠两把锁，一把是维护在 Broker 端，一把维护在消费者客户端。Broker 端有一个 RebalanceLockManager 服务，其内部维护了一个 mqLockTable 消息队列锁表: public class RebalanceLockManager { private final ConcurrentMap> mqLockTable = new ConcurrentHashMap>(1024); } 在有序消费的时候，Broker 需要确保任何一个队列在任何时候都只有一个客户端在消费它，都在被一个客户端所锁定。当客户端在本地根据消息队列构建 PullRequest 之前，会与 Broker 沟通尝试锁定这个队列，另外当进行有序消费的时候，客户端也会周期性地 (默认是 20 秒) 锁定所有当前需要消费的消息队列: public class ConsumeMessageOrderlyService implements ConsumeMessageService { public void start() { if (MessageModel.CLUSTERING.equals(ConsumeMessageOrderlyService.this.defaultMQPushConsumerImpl.messageModel())) { this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { ConsumeMessageOrderlyService.this.lockMQPeriodically(); } }, 1000 * 1, ProcessQueue.REBALANCE_LOCK_INTERVAL, TimeUnit.MILLISECONDS); } } } 由上述这段代码也能看出，只在集群模式下才会周期性地锁定 Broker 端的消息队列，因此在广播模式下是不支持进行有序消费的。 而在 Broker 这端，每个客户端所锁定的消息队列对应的锁项 LogEntry 有一个上次锁定时的时间戳，当超过锁的超时时间 (默认是 60 秒) 后，也会判定这个客户端已经不再持有这把锁，以让其他客户端能够有序消费这个队列。 在前面我们说到过 RebalanceService 均衡服务会定时地依据不同消费者数量分配消费队列。我们假设 Consumer-1 消费者客户端一开始需要消费 3 个消费队列，这个时候又加入了 Consumer-2 消费者客户端，并且分配到了 MessageQueue-2 消费队列。当 Consumer-1 内部的均衡服务检测到当前消费队列需要移除 MessageQueue-2 队列，这个时候，会首先解除 Broker 端的锁，确保新加入的 Consumer-2 消费者客户端能够成功锁住这个队列，以进行有序消费。 public abstract class RebalanceImpl { private boolean updateProcessQueueTableInRebalance(final String topic, final Set mqSet, final boolean isOrder) { while (it.hasNext()) { // ... if (mq.getTopic().equals(topic)) { // 当前客户端不需要处理这个消息队列了 if (!mqSet.contains(mq)) { pq.setDropped(true); // 解锁 if (this.removeUnnecessaryMessageQueue(mq, pq)) { // ... } } // ... } } } } 消费者客户端每一次拉取消息请求，如果有发现新的消息，那么都会将这些消息封装为 ConsumeRequest 来喂给消费线程池，以待消费。如果消息特别多，这样一个队列可能有多个消费请求正在等待客户端消费，用户可能会先消费偏移量大的消息，后消费偏移量小的消息。所以消费同一队列的时候，需要一把锁以消费请求顺序化: public class ConsumeMessageOrderlyService implements ConsumeMessageService { class ConsumeRequest implements Runnable { @Override public void run() { final Object objLock = messageQueueLock.fetchLockObject(this.messageQueue); synchronized (objLock) { // ... } } } } RocketMQ 的消息树是用 TreeMap 实现的，其内部基于消息偏移量维护了消息的有序性。每次消费请求都会从消息树中拿取偏移量最小的几条消息 (默认为 1 条)给用户，以此来达到有序消费的目的: public class ConsumeMessageOrderlyService implements ConsumeMessageService { class ConsumeRequest implements Runnable { @Override public void run() { // ... final int consumeBatchSize = ConsumeMessageOrderlyService.this .defaultMQPushConsumer .getConsumeMessageBatchMaxSize(); List msgs = this.processQueue.takeMessags(consumeBatchSize); } } } "},"doc/threadpool/":{"url":"doc/threadpool/","title":"线程池","keywords":"","body":"线程池是非常重要的工具，如果你要成为一个好的工程师，还是得比较好地掌握这个知识，很多线上问题都是因为没有用好线程池导致的。 我相信大家都看过很多的关于线程池的文章，基本上也是面试的时候必问的，如果你在看过很多文章以后，还是一知半解的，那希望这篇文章能让你真正的掌握好 Java 线程池。 总览 开篇来一些废话。下图是 java 线程池几个相关类的继承结构： 先简单说说这个继承结构，Executor 位于最顶层，也是最简单的，就一个 execute(Runnable runnable) 接口方法定义。 ExecutorService 也是接口，在 Executor 接口的基础上添加了很多的接口方法，所以一般来说我们会使用这个接口。 然后再下来一层是 AbstractExecutorService，从名字我们就知道，这是抽象类，这里实现了非常有用的一些方法供子类直接使用，之后我们再细说。 然后才到我们的重点部分 ThreadPoolExecutor 类，这个类提供了关于线程池所需的非常丰富的功能。 另外，我们还涉及到下图中的这些类： 同在并发包中的 Executors 类，类名中带字母 s，我们猜到这个是工具类，里面的方法都是静态方法，如以下我们最常用的用于生成 ThreadPoolExecutor 的实例的一些方法： public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); } public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } 另外，由于线程池支持获取线程执行的结果，所以，引入了 Future 接口，RunnableFuture 继承自此接口，然后我们最需要关心的就是它的实现类 FutureTask。到这里，记住这个概念，在线程池的使用过程中，我们是往线程池提交任务（task），使用过线程池的都知道，我们提交的每个任务是实现了 Runnable 接口的，其实就是先将 Runnable 的任务包装成 FutureTask，然后再提交到线程池。这样，读者才能比较容易记住 FutureTask 这个类名：它首先是一个任务（Task），然后具有 Future 接口的语义，即可以在将来（Future）得到执行的结果。 当然，线程池中的 BlockingQueue 也是非常重要的概念，如果线程数达到 corePoolSize，我们的每个任务会提交到等待队列中，等待线程池中的线程来取任务并执行。这里的 BlockingQueue 通常我们使用其实现类 LinkedBlockingQueue、ArrayBlockingQueue 和 SynchronousQueue，每个实现类都有不同的特征，使用场景之后会慢慢分析。想要详细了解各个 BlockingQueue 的读者，可以参考我的前面的一篇对 BlockingQueue 的各个实现类进行详细分析的文章。 把事情说完整：除了上面说的这些类外，还有一个很重要的类，就是定时任务实现类 ScheduledThreadPoolExecutor，它继承自本文要重点讲解的 ThreadPoolExecutor，用于实现定时执行。不过本文不会介绍它的实现，我相信读者看完本文后可以比较容易地看懂它的源码。 以上就是本文要介绍的知识，废话不多说，开始进入正文。 Executor 接口 /* * @since 1.5 * @author Doug Lea */ public interface Executor { void execute(Runnable command); } 我们可以看到 Executor 接口非常简单，就一个 void execute(Runnable command) 方法，代表提交一个任务。为了让大家理解 java 线程池的整个设计方案，我会按照 Doug Lea 的设计思路来多说一些相关的东西。 我们经常这样启动一个线程： new Thread(new Runnable(){ // do something }).start(); 用了线程池 Executor 后就可以像下面这么使用： Executor executor = anExecutor; executor.execute(new RunnableTask1()); executor.execute(new RunnableTask2()); 如果我们希望线程池同步执行每一个任务，我们可以这么实现这个接口： class DirectExecutor implements Executor { public void execute(Runnable r) { r.run();// 这里不是用的new Thread(r).start()，也就是说没有启动任何一个新的线程。 } } 我们希望每个任务提交进来后，直接启动一个新的线程来执行这个任务，我们可以这么实现： class ThreadPerTaskExecutor implements Executor { public void execute(Runnable r) { new Thread(r).start(); // 每个任务都用一个新的线程来执行 } } 我们再来看下怎么组合两个 Executor 来使用，下面这个实现是将所有的任务都加到一个 queue 中，然后从 queue 中取任务，交给真正的执行器执行，这里采用 synchronized 进行并发控制： class SerialExecutor implements Executor { // 任务队列 final Queue tasks = new ArrayDeque(); // 这个才是真正的执行器 final Executor executor; // 当前正在执行的任务 Runnable active; // 初始化的时候，指定执行器 SerialExecutor(Executor executor) { this.executor = executor; } // 添加任务到线程池: 将任务添加到任务队列，scheduleNext 触发执行器去任务队列取任务 public synchronized void execute(final Runnable r) { tasks.offer(new Runnable() { public void run() { try { r.run(); } finally { scheduleNext(); } } }); if (active == null) { scheduleNext(); } } protected synchronized void scheduleNext() { if ((active = tasks.poll()) != null) { // 具体的执行转给真正的执行器 executor executor.execute(active); } } } 当然了，Executor 这个接口只有提交任务的功能，太简单了，我们想要更丰富的功能，比如我们想知道执行结果、我们想知道当前线程池有多少个线程活着、已经完成了多少任务等等，这些都是这个接口的不足的地方。接下来我们要介绍的是继承自 Executor 接口的 ExecutorService 接口，这个接口提供了比较丰富的功能，也是我们最常使用到的接口。 ExecutorService 一般我们定义一个线程池的时候，往往都是使用这个接口： ExecutorService executor = Executors.newFixedThreadPool(args...); ExecutorService executor = Executors.newCachedThreadPool(args...); 因为这个接口中定义的一系列方法大部分情况下已经可以满足我们的需要了。 那么我们简单初略地来看一下这个接口中都有哪些方法： public interface ExecutorService extends Executor { // 关闭线程池，已提交的任务继续执行，不接受继续提交新任务 void shutdown(); // 关闭线程池，尝试停止正在执行的所有任务，不接受继续提交新任务 // 它和前面的方法相比，加了一个单词“now”，区别在于它会去停止当前正在进行的任务 List shutdownNow(); // 线程池是否已关闭 boolean isShutdown(); // 如果调用了 shutdown() 或 shutdownNow() 方法后，所有任务结束了，那么返回true // 这个方法必须在调用shutdown或shutdownNow方法之后调用才会返回true boolean isTerminated(); // 等待所有任务完成，并设置超时时间 // 我们这么理解，实际应用中是，先调用 shutdown 或 shutdownNow， // 然后再调这个方法等待所有的线程真正地完成，返回值意味着有没有超时 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个 Callable 任务 Future submit(Callable task); // 提交一个 Runnable 任务，第二个参数将会放到 Future 中，作为返回值， // 因为 Runnable 的 run 方法本身并不返回任何东西 Future submit(Runnable task, T result); // 提交一个 Runnable 任务 Future submit(Runnable task); // 执行所有任务，返回 Future 类型的一个 list List> invokeAll(Collection> tasks) throws InterruptedException; // 也是执行所有任务，但是这里设置了超时时间 List> invokeAll(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException; // 只有其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果 T invokeAny(Collection> tasks) throws InterruptedException, ExecutionException; // 同上一个方法，只有其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果， // 不过这个带超时，超过指定的时间，抛出 TimeoutException 异常 T invokeAny(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } 这些方法都很好理解，一个简单的线程池主要就是这些功能，能提交任务，能获取结果，能关闭线程池，这也是为什么我们经常用这个接口的原因。 FutureTask 在继续往下层介绍 ExecutorService 的实现类之前，我们先来说说相关的类 FutureTask。 Future Runnable \\ / \\ / RunnableFuture | | FutureTask FutureTask 通过 RunnableFuture 间接实现了 Runnable 接口， 所以每个 Runnable 通常都先包装成 FutureTask， 然后调用 executor.execute(Runnable command) 将其提交给线程池 我们知道，Runnable 的 void run() 方法是没有返回值的，所以，通常，如果我们需要的话，会在 submit 中指定第二个参数作为返回值： Future submit(Runnable task, T result); 其实到时候会通过这两个参数，将其包装成 Callable。它和 Runnable 的区别在于 run() 没有返回值，而 Callable 的 call() 方法有返回值，同时，如果运行出现异常，call() 方法会抛出异常。 public interface Callable { V call() throws Exception; } 在这里，就不展开说 FutureTask 类了，因为本文篇幅本来就够大了，这里我们需要知道怎么用就行了。 下面，我们来看看 ExecutorService 的抽象实现 AbstractExecutorService 。 AbstractExecutorService AbstractExecutorService 抽象类派生自 ExecutorService 接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 这个抽象类实现了 invokeAny 方法和 invokeAll 方法，这里的两个 newTaskFor 方法也比较有用，用于将任务包装成 FutureTask。定义于最上层接口 Executor中的 void execute(Runnable command) 由于不需要获取结果，不会进行 FutureTask 的包装。 需要获取结果（FutureTask），用 submit 方法，不需要获取结果，可以用 execute 方法。 下面，我将一行一行源码地来分析这个类，跟着源码来看看其实现吧： Tips: invokeAny 和 invokeAll 方法占了这整个类的绝大多数篇幅，读者可以选择适当跳过，因为它们可能在你的实践中使用的频次比较低，而且它们不带有承前启后的作用，不用担心会漏掉什么导致看不懂后面的代码。 public abstract class AbstractExecutorService implements ExecutorService { // RunnableFuture 是用于获取执行结果的，我们常用它的子类 FutureTask // 下面两个 newTaskFor 方法用于将我们的任务包装成 FutureTask 提交到线程池中执行 protected RunnableFuture newTaskFor(Runnable runnable, T value) { return new FutureTask(runnable, value); } protected RunnableFuture newTaskFor(Callable callable) { return new FutureTask(callable); } // 提交任务 public Future submit(Runnable task) { if (task == null) throw new NullPointerException(); // 1. 将任务包装成 FutureTask RunnableFuture ftask = newTaskFor(task, null); // 2. 交给执行器执行，execute 方法由具体的子类来实现 // 前面也说了，FutureTask 间接实现了Runnable 接口。 execute(ftask); return ftask; } public Future submit(Runnable task, T result) { if (task == null) throw new NullPointerException(); // 1. 将任务包装成 FutureTask RunnableFuture ftask = newTaskFor(task, result); // 2. 交给执行器执行 execute(ftask); return ftask; } public Future submit(Callable task) { if (task == null) throw new NullPointerException(); // 1. 将任务包装成 FutureTask RunnableFuture ftask = newTaskFor(task); // 2. 交给执行器执行 execute(ftask); return ftask; } // 此方法目的：将 tasks 集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了 // 第二个参数 timed 代表是否设置超时机制，超时时间为第三个参数， // 如果 timed 为 true，同时超时了还没有一个线程返回结果，那么抛出 TimeoutException 异常 private T doInvokeAny(Collection> tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException { if (tasks == null) throw new NullPointerException(); // 任务数 int ntasks = tasks.size(); if (ntasks == 0) throw new IllegalArgumentException(); // List> futures= new ArrayList>(ntasks); // ExecutorCompletionService 不是一个真正的执行器，参数 this 才是真正的执行器 // 它对执行器进行了包装，每个任务结束后，将结果保存到内部的一个 completionQueue 队列中 // 这也是为什么这个类的名字里面有个 Completion 的原因吧。 ExecutorCompletionService ecs = new ExecutorCompletionService(this); try { // 用于保存异常信息，此方法如果没有得到任何有效的结果，那么我们可以抛出最后得到的一个异常 ExecutionException ee = null; long lastTime = timed ? System.nanoTime() : 0; Iterator> it = tasks.iterator(); // 首先先提交一个任务，后面的任务到下面的 for 循环一个个提交 futures.add(ecs.submit(it.next())); // 提交了一个任务，所以任务数量减 1 --ntasks; // 正在执行的任务数(提交的时候 +1，任务结束的时候 -1) int active = 1; for (;;) { // ecs 上面说了，其内部有一个 completionQueue 用于保存执行完成的结果 // BlockingQueue 的 poll 方法不阻塞，返回 null 代表队列为空 Future f = ecs.poll(); // 为 null，说明刚刚提交的第一个线程还没有执行完成 // 在前面先提交一个任务，加上这里做一次检查，也是为了提高性能 if (f == null) { if (ntasks > 0) { --ntasks; futures.add(ecs.submit(it.next())); ++active; } // 这里是 else if，不是 if。这里说明，没有任务了，同时 active 为 0 说明 // 任务都执行完成了。其实我也没理解为什么这里做一次 break？ // 因为我认为 active 为 0 的情况，必然从下面的 f.get() 返回了 // 2018-02-23 感谢读者 newmicro 的 comment， // 这里的 active == 0，说明所有的任务都执行失败，那么这里是 for 循环出口 else if (active == 0) break; // 这里也是 else if。这里说的是，没有任务了，但是设置了超时时间，这里检测是否超时 else if (timed) { // 带等待的 poll 方法 f = ecs.poll(nanos, TimeUnit.NANOSECONDS); // 如果已经超时，抛出 TimeoutException 异常，这整个方法就结束了 if (f == null) throw new TimeoutException(); long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; } // 这里是 else。说明，没有任务需要提交，但是池中的任务没有完成，还没有超时(如果设置了超时) // take() 方法会阻塞，直到有元素返回，说明有任务结束了 else f = ecs.take(); } /* * 我感觉上面这一段并不是很好理解，这里简单说下。 * 1. 首先，这在一个 for 循环中，我们设想每一个任务都没那么快结束， * 那么，每一次都会进到第一个分支，进行提交任务，直到将所有的任务都提交了 * 2. 任务都提交完成后，如果设置了超时，那么 for 循环其实进入了“一直检测是否超时” 这件事情上 * 3. 如果没有设置超时机制，那么不必要检测超时，那就会阻塞在 ecs.take() 方法上， 等待获取第一个执行结果 * 4. 如果所有的任务都执行失败，也就是说 future 都返回了， 但是 f.get() 抛出异常，那么从 active == 0 分支出去(感谢 newmicro 提出) // 当然，这个需要看下面的 if 分支。 */ // 有任务结束了 if (f != null) { --active; try { // 返回执行结果，如果有异常，都包装成 ExecutionException return f.get(); } catch (ExecutionException eex) { ee = eex; } catch (RuntimeException rex) { ee = new ExecutionException(rex); } } }// 注意看 for 循环的范围，一直到这里 if (ee == null) ee = new ExecutionException(); throw ee; } finally { // 方法退出之前，取消其他的任务 for (Future f : futures) f.cancel(true); } } public T invokeAny(Collection> tasks) throws InterruptedException, ExecutionException { try { return doInvokeAny(tasks, false, 0); } catch (TimeoutException cannotHappen) { assert false; return null; } } public T invokeAny(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException { return doInvokeAny(tasks, true, unit.toNanos(timeout)); } // 执行所有的任务，返回任务结果。 // 先不要看这个方法，我们先想想，其实我们自己提交任务到线程池，也是想要线程池执行所有的任务 // 只不过，我们是每次 submit 一个任务，这里以一个集合作为参数提交 public List> invokeAll(Collection> tasks) throws InterruptedException { if (tasks == null) throw new NullPointerException(); List> futures = new ArrayList>(tasks.size()); boolean done = false; try { // 这个很简单 for (Callable t : tasks) { // 包装成 FutureTask RunnableFuture f = newTaskFor(t); futures.add(f); // 提交任务 execute(f); } for (Future f : futures) { if (!f.isDone()) { try { // 这是一个阻塞方法，直到获取到值，或抛出了异常 // 这里有个小细节，其实 get 方法签名上是会抛出 InterruptedException 的 // 可是这里没有进行处理，而是抛给外层去了。此异常发生于还没执行完的任务被取消了 f.get(); } catch (CancellationException ignore) { } catch (ExecutionException ignore) { } } } done = true; // 这个方法返回，不像其他的场景，返回 List，其实执行结果还没出来 // 这个方法返回是真正的返回，任务都结束了 return futures; } finally { // 为什么要这个？就是上面说的有异常的情况 if (!done) for (Future f : futures) f.cancel(true); } } // 带超时的 invokeAll，我们找不同吧 public List> invokeAll(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException { if (tasks == null || unit == null) throw new NullPointerException(); long nanos = unit.toNanos(timeout); List> futures = new ArrayList>(tasks.size()); boolean done = false; try { for (Callable t : tasks) futures.add(newTaskFor(t)); long lastTime = System.nanoTime(); Iterator> it = futures.iterator(); // 每提交一个任务，检测一次是否超时 while (it.hasNext()) { execute((Runnable)(it.next())); long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; // 超时 if (nanos f : futures) { if (!f.isDone()) { if (nanos f : futures) f.cancel(true); } } } 到这里，我们发现，这个抽象类包装了一些基本的方法，可是像 submit、invokeAny、invokeAll 等方法，它们都没有真正开启线程来执行任务，它们都只是在方法内部调用了 execute 方法，所以最重要的 execute(Runnable runnable) 方法还没出现，需要等具体执行器来实现这个最重要的部分，这里我们要说的就是 ThreadPoolExecutor 类了。 鉴于本文的篇幅，我觉得看到这里的读者应该已经不多了，大家都习惯了快餐文化。我写的每篇文章都力求让读者可以通过我的一篇文章而对相关内容有全面的了解，所以篇幅不免长了些。 ThreadPoolExecutor ThreadPoolExecutor 是 JDK 中的线程池实现，这个类实现了一个线程池需要的各个方法，它实现了任务提交、线程管理、监控等等方法。 我们可以基于它来进行业务上的扩展，以实现我们需要的其他功能，比如实现定时任务的类 ScheduledThreadPoolExecutor 就继承自 ThreadPoolExecutor。当然，这不是本文关注的重点，下面，还是赶紧进行源码分析吧。 首先，我们来看看线程池实现中的几个概念和处理流程。 我们先回顾下提交任务的几个方法： public Future submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture ftask = newTaskFor(task, null); execute(ftask); return ftask; } public Future submit(Runnable task, T result) { if (task == null) throw new NullPointerException(); RunnableFuture ftask = newTaskFor(task, result); execute(ftask); return ftask; } public Future submit(Callable task) { if (task == null) throw new NullPointerException(); RunnableFuture ftask = newTaskFor(task); execute(ftask); return ftask; } 一个最基本的概念是，submit 方法中，参数是 Runnable 类型（也有Callable 类型），这个参数不是用于 new Thread(runnable).start() 中的，此处的这个参数不是用于启动线程的，这里指的是任务，任务要做的事情是 run() 方法里面定义的或 Callable 中的 call() 方法里面定义的。 初学者往往会搞混这个，因为 Runnable 总是在各个地方出现，经常把一个 Runnable 包到另一个 Runnable 中。请把它想象成有个 Task 接口，这个接口里面有一个 run() 方法。 我们回过神来继续往下看，我画了一个简单的示意图来描述线程池中的一些主要的构件： 当然，上图没有考虑队列是否有界，提交任务时队列满了怎么办？什么情况下会创建新的线程？提交任务时线程池满了怎么办？空闲线程怎么关掉？这些问题下面我们会一一解决。 我们经常会使用 Executors 这个工具类来快速构造一个线程池，对于初学者而言，这种工具类是很有用的，开发者不需要关注太多的细节，只要知道自己需要一个线程池，仅仅提供必需的参数就可以了，其他参数都采用作者提供的默认值。 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); } 这里先不说有什么区别，它们最终都会导向这个构造方法： public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize 基本上，上面的构造方法中列出了我们最需要关心的几个属性了，下面逐个介绍下构造方法中出现的这几个属性： corePoolSize 核心线程数，不要抠字眼，反正先记着有这么个属性就可以了。 maximumPoolSize 最大线程数，线程池允许创建的最大线程数。 workQueue 任务队列，BlockingQueue 接口的某个实现（常使用 ArrayBlockingQueue 和 LinkedBlockingQueue）。 keepAliveTime 空闲线程的保活时间，如果某线程的空闲时间超过这个值都没有任务给它做，那么可以被关闭了。注意这个值并不会对所有线程起作用，如果线程池中的线程数少于等于核心线程数 corePoolSize，那么这些线程不会因为空闲太长时间而被关闭，当然，也可以通过调用 allowCoreThreadTimeOut(true)使核心线程数内的线程也可以被回收。 threadFactory 用于生成线程，一般我们可以用默认的就可以了。通常，我们可以通过它将我们的线程的名字设置得比较可读一些，如 Message-Thread-1， Message-Thread-2 类似这样。 handler： 当线程池已经满了，但是又有新的任务提交的时候，该采取什么策略由这个来指定。有几种方式可供选择，像抛出异常、直接拒绝然后返回等，也可以自己实现相应的接口实现自己的逻辑，这个之后再说。 除了上面几个属性外，我们再看看其他重要的属性。 Doug Lea 采用一个 32 位的整数来存放线程池的状态和当前池中的线程数，其中高 3 位用于存放线程池状态，低 29 位表示线程数（即使只有 29 位，也已经不小了，大概 5 亿多，现在还没有哪个机器能起这么多线程的吧）。我们知道，java 语言在整数编码上是统一的，都是采用补码的形式，下面是简单的移位操作和布尔操作，都是挺简单的。 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); // 这里 COUNT_BITS 设置为 29(32-3)，意味着前三位用于存放线程状态，后29位用于存放线程数 // 很多初学者很喜欢在自己的代码中写很多 29 这种数字，或者某个特殊的字符串，然后分布在各个地方，这是非常糟糕的 private static final int COUNT_BITS = Integer.SIZE - 3; // 000 11111111111111111111111111111 // 这里得到的是 29 个 1，也就是说线程池的最大线程数是 2^29-1=536870911 // 以我们现在计算机的实际情况，这个数量还是够用的 private static final int CAPACITY = (1 = s; } private static boolean isRunning(int c) { return c 上面就是对一个整数的简单的位操作，几个操作方法将会在后面的源码中一直出现，所以读者最好把方法名字和其代表的功能记住，看源码的时候也就不需要来来回回翻了。 在这里，介绍下线程池中的各个状态和状态变化的转换过程： RUNNING：这个没什么好说的，这是最正常的状态：接受新的任务，处理等待队列中的任务 SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务 STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程 TIDYING：所有的任务都销毁了，workCount 为 0。线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated() TERMINATED：terminated() 方法结束后，线程池的状态就会变成这个 RUNNING 定义为 -1，SHUTDOWN 定义为 0，其他的都比 0 大，所以等于 0 的时候不能提交任务，大于 0 的话，连正在执行的任务也需要中断。 看了这几种状态的介绍，读者大体也可以猜到十之八九的状态转换了，各个状态的转换过程有以下几种： RUNNING -> SHUTDOWN：当调用了 shutdown() 后，会发生这个状态转换，这也是最重要的 (RUNNING or SHUTDOWN) -> STOP：当调用 shutdownNow() 后，会发生这个状态转换，这下要清楚 shutDown() 和 shutDownNow() 的区别了 SHUTDOWN -> TIDYING：当任务队列和线程池都清空后，会由 SHUTDOWN 转换为 TIDYING STOP -> TIDYING：当任务队列清空后，发生这个转换 TIDYING -> TERMINATED：这个前面说了，当 terminated() 方法结束后 上面的几个记住核心的就可以了，尤其第一个和第二个。 另外，我们还要看看一个内部类 Worker，因为 Doug Lea 把线程池中的线程包装成了一个个 Worker，翻译成工人，就是线程池中做任务的线程。所以到这里，我们知道任务是 Runnable（内部变量名叫 task 或 command），线程是 Worker。 Worker 这里又用到了抽象类 AbstractQueuedSynchronizer。题外话，AQS 在并发中真的是到处出现，而且非常容易使用，写少量的代码就能实现自己需要的同步方式（对 AQS 源码感兴趣的读者请参看我之前写的几篇文章）。 private final class Worker extends AbstractQueuedSynchronizer implements Runnable { private static final long serialVersionUID = 6138294804551838833L; // 这个是真正的线程，任务靠你啦 final Thread thread; // 前面说了，这里的 Runnable 是任务。为什么叫 firstTask？因为在创建线程的时候，如果同时指定了 // 这个线程起来以后需要执行的第一个任务，那么第一个任务就是存放在这里的(线程可不止执行这一个任务) // 当然了，也可以为 null，这样线程起来了，自己到任务队列（BlockingQueue）中取任务（getTask 方法）就行了 Runnable firstTask; // 用于存放此线程完成的任务数，注意了，这里用了 volatile，保证可见性 volatile long completedTasks; // Worker 只有这一个构造方法，传入 firstTask，也可以传 null Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 调用 ThreadFactory 来创建一个新的线程 this.thread = getThreadFactory().newThread(this); } // 这里调用了外部类的 runWorker 方法 public void run() { runWorker(this); } ...// 其他几个方法没什么好看的，就是用 AQS 操作，来获取这个线程的执行权，用了独占锁 } 前面虽然啰嗦，但是简单。有了上面的这些基础后，我们终于可以看看 ThreadPoolExecutor 的 execute 方法了，前面源码分析的时候也说了，各种方法都最终依赖于 execute 方法： public void execute(Runnable command) { if (command == null) throw new NullPointerException(); // 前面说的那个表示 “线程池状态” 和 “线程数” 的整数 int c = ctl.get(); // 如果当前线程数少于核心线程数，那么直接添加一个 worker 来执行任务， // 创建一个新的线程，并把当前任务 command 作为这个线程的第一个任务(firstTask) if (workerCountOf(c) 对创建线程的错误理解：如果线程数少于 corePoolSize，创建一个线程，如果线程数在 [corePoolSize, maximumPoolSize] 之间那么可以创建线程或复用空闲线程，keepAliveTime 对这个区间的线程有效。 从上面的几个分支，我们就可以看出，上面的这段话是错误的。 上面这些一时半会也不可能全部消化搞定，我们先继续往下吧，到时候再回头看几遍。 这个方法非常重要 addWorker(Runnable firstTask, boolean core) 方法，我们看看它是怎么创建新的线程的： // 第一个参数是准备提交给这个线程执行的任务，之前说了，可以为 null // 第二个参数为 true 代表使用核心线程数 corePoolSize 作为创建线程的界限，也就说创建这个线程的时候， // 如果线程池中的线程总数已经达到 corePoolSize，那么不能响应这次创建线程的请求 // 如果是 false，代表使用最大线程数 maximumPoolSize 作为界限 private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 这个非常不好理解 // 如果线程池已关闭，并满足以下条件之一，那么不创建新的 worker： // 1. 线程池状态大于 SHUTDOWN，其实也就是 STOP, TIDYING, 或 TERMINATED // 2. firstTask != null // 3. workQueue.isEmpty() // 简单分析下： // 还是状态控制的问题，当线程池处于 SHUTDOWN 的时候，不允许提交任务，但是已有的任务继续执行 // 当状态大于 SHUTDOWN 时，不允许提交任务，且中断正在执行的任务 // 多说一句：如果线程池处于 SHUTDOWN，但是 firstTask 为 null，且 workQueue 非空，那么是允许创建 worker 的 // 这是因为 SHUTDOWN 的语义：不允许提交新的任务，但是要把已经进入到 workQueue 的任务执行完，所以在满足条件的基础上，是允许创建新的 Worker 的 if (rs >= SHUTDOWN && ! (rs == SHUTDOWN && firstTask == null && ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); if (wc >= CAPACITY || wc >= (core ? corePoolSize : maximumPoolSize)) return false; // 如果成功，那么就是所有创建线程前的条件校验都满足了，准备创建线程执行任务了 // 这里失败的话，说明有其他线程也在尝试往线程池中创建线程 if (compareAndIncrementWorkerCount(c)) break retry; // 由于有并发，重新再读取一下 ctl c = ctl.get(); // 正常如果是 CAS 失败的话，进到下一个里层的for循环就可以了 // 可是如果是因为其他线程的操作，导致线程池的状态发生了变更，如有其他线程关闭了这个线程池 // 那么需要回到外层的for循环 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } /* * 到这里，我们认为在当前这个时刻，可以开始创建线程来执行任务了， * 因为该校验的都校验了，至于以后会发生什么，那是以后的事，至少当前是满足条件的 */ // worker 是否已经启动 boolean workerStarted = false; // 是否已将这个 worker 添加到 workers 这个 HashSet 中 boolean workerAdded = false; Worker w = null; try { final ReentrantLock mainLock = this.mainLock; // 把 firstTask 传给 worker 的构造方法 w = new Worker(firstTask); // 取 worker 中的线程对象，之前说了，Worker的构造方法会调用 ThreadFactory 来创建一个新的线程 final Thread t = w.thread; if (t != null) { // 这个是整个线程池的全局锁，持有这个锁才能让下面的操作“顺理成章”， // 因为关闭一个线程池需要这个锁，至少我持有锁的期间，线程池不会被关闭 mainLock.lock(); try { int c = ctl.get(); int rs = runStateOf(c); // 小于 SHUTTDOWN 那就是 RUNNING，这个自不必说，是最正常的情况 // 如果等于 SHUTDOWN，前面说了，不接受新的任务，但是会继续执行等待队列中的任务 if (rs largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } // 添加成功的话，启动这个线程 if (workerAdded) { // 启动线程 t.start(); workerStarted = true; } } } finally { // 如果线程没有启动，需要做一些清理工作，如前面 workCount 加了 1，将其减掉 if (! workerStarted) addWorkerFailed(w); } // 返回线程是否启动成功 return workerStarted; } 简单看下 addWorkFailed 的处理： // workers 中删除掉相应的 worker // workCount 减 1 private void addWorkerFailed(Worker w) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { if (w != null) workers.remove(w); decrementWorkerCount(); // rechecks for termination, in case the existence of this worker was holding up termination tryTerminate(); } finally { mainLock.unlock(); } } 回过头来，继续往下走。我们知道，worker 中的线程 start 后，其 run 方法会调用 runWorker 方法： // Worker 类的 run() 方法 public void run() { runWorker(this); } 继续往下看 runWorker 方法： // 此方法由 worker 线程启动后调用，这里用一个 while 循环来不断地从等待队列中获取任务并执行 // 前面说了，worker 在初始化的时候，可以指定 firstTask，那么第一个任务也就可以不需要从队列中获取 final void runWorker(Worker w) { // Thread wt = Thread.currentThread(); // 该线程的第一个任务(如果有的话) Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try { // 循环调用 getTask 获取任务 while (task != null || (task = getTask()) != null) { w.lock(); // 如果线程池状态大于等于 STOP，那么意味着该线程也要中断 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() && runStateAtLeast(ctl.get(), STOP))) && !wt.isInterrupted()) wt.interrupt(); try { // 这是一个钩子方法，留给需要的子类实现 beforeExecute(wt, task); Throwable thrown = null; try { // 到这里终于可以执行任务了 task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { // 这里不允许抛出 Throwable，所以转换为 Error thrown = x; throw new Error(x); } finally { // 也是一个钩子方法，将 task 和异常作为参数，留给需要的子类实现 afterExecute(task, thrown); } } finally { // 置空 task，准备 getTask 获取下一个任务 task = null; // 累加完成的任务数 w.completedTasks++; // 释放掉 worker 的独占锁 w.unlock(); } } completedAbruptly = false; } finally { // 如果到这里，需要执行线程关闭： // 1. 说明 getTask 返回 null，也就是说，队列中已经没有任务需要执行了，执行关闭 // 2. 任务执行过程中发生了异常 // 第一种情况，已经在代码处理了将 workCount 减 1，这个在 getTask 方法分析中会说 // 第二种情况，workCount 没有进行处理，所以需要在 processWorkerExit 中处理 // 限于篇幅，我不准备分析这个方法了，感兴趣的读者请自行分析源码 processWorkerExit(w, completedAbruptly); } } 我们看看 getTask() 是怎么获取任务的，这个方法写得真的很好，每一行都很简单，组合起来却所有的情况都想好了： // 此方法有三种可能： // 1. 阻塞直到获取到任务返回。我们知道，默认 corePoolSize 之内的线程是不会被回收的， // 它们会一直等待任务 // 2. 超时退出。keepAliveTime 起作用的时候，也就是如果这么多时间内都没有任务，那么应该执行关闭 // 3. 如果发生了以下条件，此方法必须返回 null: // - 池中有大于 maximumPoolSize 个 workers 存在(通过调用 setMaximumPoolSize 进行设置) // - 线程池处于 SHUTDOWN，而且 workQueue 是空的，前面说了，这种不再接受新的任务 // - 线程池处于 STOP，不仅不接受新的线程，连 workQueue 中的线程也不再执行 private Runnable getTask() { boolean timedOut = false; // Did the last poll() time out? retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 两种可能 // 1. rs == SHUTDOWN && workQueue.isEmpty() // 2. rs >= STOP if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty())) { // CAS 操作，减少工作线程数 decrementWorkerCount(); return null; } boolean timed; // Are workers subject to culling? for (;;) { int wc = workerCountOf(c); // 允许核心线程数内的线程回收，或当前线程数超过了核心线程数，那么有可能发生超时关闭 timed = allowCoreThreadTimeOut || wc > corePoolSize; // 这里 break，是为了不往下执行后一个 if (compareAndDecrementWorkerCount(c)) // 两个 if 一起看：如果当前线程数 wc > maximumPoolSize，或者超时，都返回 null // 那这里的问题来了，wc > maximumPoolSize 的情况，为什么要返回 null？ // 换句话说，返回 null 意味着关闭线程。 // 那是因为有可能开发者调用了 setMaximumPoolSize() 将线程池的 maximumPoolSize 调小了，那么多余的 Worker 就需要被关闭 if (wc 到这里，基本上也说完了整个流程，读者这个时候应该回到 execute(Runnable command) 方法，看看各个分支，我把代码贴过来一下： public void execute(Runnable command) { if (command == null) throw new NullPointerException(); // 前面说的那个表示 “线程池状态” 和 “线程数” 的整数 int c = ctl.get(); // 如果当前线程数少于核心线程数，那么直接添加一个 worker 来执行任务， // 创建一个新的线程，并把当前任务 command 作为这个线程的第一个任务(firstTask) if (workerCountOf(c) 上面各个分支中，有两种情况会调用 reject(command) 来处理任务，因为按照正常的流程，线程池此时不能接受这个任务，所以需要执行我们的拒绝策略。接下来，我们说一说 ThreadPoolExecutor 中的拒绝策略。 final void reject(Runnable command) { // 执行拒绝策略 handler.rejectedExecution(command, this); } 此处的 handler 我们需要在构造线程池的时候就传入这个参数，它是 RejectedExecutionHandler 的实例。 RejectedExecutionHandler 在 ThreadPoolExecutor 中有四个已经定义好的实现类可供我们直接使用，当然，我们也可以实现自己的策略，不过一般也没有必要。 // 只要线程池没有被关闭，那么由提交任务的线程自己来执行这个任务。 public static class CallerRunsPolicy implements RejectedExecutionHandler { public CallerRunsPolicy() { } public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { r.run(); } } } // 不管怎样，直接抛出 RejectedExecutionException 异常 // 这个是默认的策略，如果我们构造线程池的时候不传相应的 handler 的话，那就会指定使用这个 public static class AbortPolicy implements RejectedExecutionHandler { public AbortPolicy() { } public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { throw new RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); } } // 不做任何处理，直接忽略掉这个任务 public static class DiscardPolicy implements RejectedExecutionHandler { public DiscardPolicy() { } public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { } } // 这个相对霸道一点，如果线程池没有被关闭的话， // 把队列队头的任务(也就是等待了最长时间的)直接扔掉，然后提交这个任务到等待队列中 public static class DiscardOldestPolicy implements RejectedExecutionHandler { public DiscardOldestPolicy() { } public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { e.getQueue().poll(); e.execute(r); } } } 到这里，ThreadPoolExecutor 的源码算是分析结束了。单纯从源码的难易程度来说，ThreadPoolExecutor 的源码还算是比较简单的，只是需要我们静下心来好好看看罢了。 Executors 这节其实也不是分析 Executors 这个类，因为它仅仅是工具类，它的所有方法都是 static 的。 生成一个固定大小的线程池： public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } 最大线程数设置为与核心线程数相等，此时 keepAliveTime 设置为 0（因为这里它是没用的，即使不为 0，线程池默认也不会回收 corePoolSize 内的线程），任务队列采用 LinkedBlockingQueue，无界队列。 过程分析：刚开始，每提交一个任务都创建一个 worker，当 worker 的数量达到 nThreads 后，不再创建新的线程，而是把任务提交到 LinkedBlockingQueue 中，而且之后线程数始终为 nThreads。 生成只有一个线程的固定线程池，这个更简单，和上面的一样，只要设置线程数为 1 就可以了： public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue())); } 生成一个需要的时候就创建新的线程，同时可以复用之前创建的线程（如果这个线程当前没有任务）的线程池： public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); } 核心线程数为 0，最大线程数为 Integer.MAX_VALUE，keepAliveTime 为 60 秒，任务队列采用 SynchronousQueue。 这种线程池对于任务可以比较快速地完成的情况有比较好的性能。如果线程空闲了 60 秒都没有任务，那么将关闭此线程并从线程池中移除。所以如果线程池空闲了很长时间也不会有问题，因为随着所有的线程都会被关闭，整个线程池不会占用任何的系统资源。 过程分析：我把 execute 方法的主体黏贴过来，让大家看得明白些。鉴于 corePoolSize 是 0，那么提交任务的时候，直接将任务提交到队列中，由于采用了 SynchronousQueue，所以如果是第一个任务提交的时候，offer 方法肯定会返回 false，因为此时没有任何 worker 对这个任务进行接收，那么将进入到最后一个分支来创建第一个 worker。之后再提交任务的话，取决于是否有空闲下来的线程对任务进行接收，如果有，会进入到第二个 if 语句块中，否则就是和第一个任务一样，进到最后的 else if 分支创建新线程。 int c = ctl.get(); // corePoolSize 为 0，所以不会进到这个 if 分支 if (workerCountOf(c) SynchronousQueue 是一个比较特殊的 BlockingQueue，其本身不储存任何元素，它有一个虚拟队列（或虚拟栈），不管读操作还是写操作，如果当前队列中存储的是与当前操作相同模式的线程，那么当前操作也进入队列中等待；如果是相反模式，则配对成功，从当前队列中取队头节点。具体的信息，可以看我的另一篇关于 BlockingQueue 的文章。 总结 java 线程池有哪些关键属性？ corePoolSize，maximumPoolSize，workQueue，keepAliveTime，rejectedExecutionHandler corePoolSize 到 maximumPoolSize 之间的线程会被回收，当然 corePoolSize 的线程也可以通过设置而得到回收（allowCoreThreadTimeOut(true)）。 workQueue 用于存放任务，添加任务的时候，如果当前线程数超过了 corePoolSize，那么往该队列中插入任务，线程池中的线程会负责到队列中拉取任务。 keepAliveTime 用于设置空闲时间，如果线程数超出了 corePoolSize，并且有些线程的空闲时间超过了这个值，会执行关闭这些线程的操作 rejectedExecutionHandler 用于处理当线程池不能执行此任务时的情况，默认有抛出 RejectedExecutionException 异常、忽略任务、使用提交任务的线程来执行此任务和将队列中等待最久的任务删除，然后提交此任务这四种策略，默认为抛出异常。 说说线程池中的线程创建时机？ 如果当前线程数少于 corePoolSize，那么提交任务的时候创建一个新的线程，并由这个线程执行这个任务； 如果当前线程数已经达到 corePoolSize，那么将提交的任务添加到队列中，等待线程池中的线程去队列中取任务； 如果队列已满，那么创建新的线程来执行任务，需要保证池中的线程数不会超过 maximumPoolSize，如果此时线程数超过了 maximumPoolSize，那么执行拒绝策略。 * 注意：如果将队列设置为无界队列，那么线程数达到 corePoolSize 后，其实线程数就不会再增长了。因为后面的任务直接往队列塞就行了，此时 maximumPoolSize 参数就没有什么意义。 Executors.newFixedThreadPool(…) 和 Executors.newCachedThreadPool() 构造出来的线程池有什么差别？ 细说太长，往上滑一点点，在 Executors 的小节进行了详尽的描述。 任务执行过程中发生异常怎么处理？ 如果某个任务执行出现异常，那么执行任务的线程会被关闭，而不是继续接收其他任务。然后会启动一个新的线程来代替它。 什么时候会执行拒绝策略？ workers 的数量达到了 corePoolSize（任务此时需要进入任务队列），任务入队成功，与此同时线程池被关闭了，而且关闭线程池并没有将这个任务出队，那么执行拒绝策略。这里说的是非常边界的问题，入队和关闭线程池并发执行，读者仔细看看 execute 方法是怎么进到第一个 reject(command) 里面的。 workers 的数量大于等于 corePoolSize，将任务加入到任务队列，可是队列满了，任务入队失败，那么准备开启新的线程，可是线程数已经达到 maximumPoolSize，那么执行拒绝策略。 怎么获取获取线程池线程结果？ 通过 Future 类的 get 方法获取结果。 本文篇幅是有点长，如果读者发现什么不对的地方，或者有需要补充的地方，请不吝提出，谢谢。 （全文完） "}}
